# DECOHERE Pipeline Documentation

This directory contains the Jupyter notebook for running the DECOHERE quantitative trading pipeline. The pipeline is designed to process financial data, generate features, select important features using SHAP, and run regression models.

## Pipeline Overview

The main notebook `pipeline.ipynb` provides an interface to run the complete pipeline with the following components:

1. **Data Loading**: Load raw financial data from the configured source
2. **Data Processing**: Clean and transform the data for analysis
3. **Feature Generation**: Create features for machine learning models
4. **Feature Selection**: Select important features using SHAP analysis
5. **Model Training**: Train and evaluate regression models
6. **Visualization**: Visualize results and model performance
7. **Results Storage**: Save processed data, features, and models

## Running the Pipeline

The pipeline can be run in three modes:
- **Day Mode**: Process a single day of data
- **Week Mode**: Process a week of data (selected date + 6 days)
- **Year Mode**: Process a year of data (selected date + 364 days)

### Steps to Run the Pipeline:

1. Start Jupyter Notebook:
   ```bash
   jupyter notebook
   ```

2. Open the `pipeline.ipynb` notebook.

3. Select the mode (day, week, or year) and date using the dropdown menus.

4. Run each cell in sequence to execute the pipeline steps.

5. View the results and visualizations generated by the pipeline.

## Pipeline Implementation Details

### Configuration

The pipeline reads configuration from YAML files in the `config/` directory:
- `config.yaml`: Main configuration file
- `day_mode.yaml`: Configuration for day mode
- `week_mode.yaml`: Configuration for week mode
- `year_mode.yaml`: Configuration for year mode

### Data Processing

The data processing step handles:
- Loading raw financial data from the configured source
- Cleaning and transforming the data
- Handling missing values and outliers
- Saving processed data to the configured output directory

The `process_data` function determines the appropriate date range based on the selected mode:
```python
# Determine date range based on mode
if mode == 'day':
    start_date = date_str
    end_date = date_str
elif mode == 'week':
    start_date = date_str
    end_date = (date + timedelta(days=6)).strftime('%Y-%m-%d')
elif mode == 'year':
    start_date = date_str
    end_date = (date + timedelta(days=364)).strftime('%Y-%m-%d')
```

### Feature Engineering

The feature engineering step:
- Generates features from the processed data
- Calculates financial ratios and metrics
- Prepares features for model training

### Feature Selection

The feature selection step:
- Uses SHAP (SHapley Additive exPlanations) to determine feature importance
- Selects the most important features based on configured thresholds
- Reduces dimensionality for more efficient model training

### Model Training and Evaluation

The model training step:
- Trains regression models on the selected features
- Evaluates model performance using cross-validation
- Calculates performance metrics (RMSE, MAE, RÂ²)

### Visualization

The visualization step:
- Plots feature importance
- Visualizes SHAP values
- Shows actual vs. predicted values
- Generates other relevant visualizations

### Results Storage

Results are saved to the `data/results/` directory, organized by mode and date:
- Processed data
- Generated features
- Selected features
- Trained models
- Performance metrics

## Notebook Generation

If you need to regenerate the notebook, you can use the `generate_pipeline_notebook.py` script:

```bash
python generate_pipeline_notebook.py
```

This will create a new version of the `pipeline.ipynb` file with all the necessary code cells.

## Troubleshooting

### Common Issues

#### Missing Parameters in DataProcessor.process_data()

If you encounter an error like:
```
TypeError: DataProcessor.process_data() missing 1 required positional argument: 'end_date'
```

This is because the `process_data()` method in the `DataProcessor` class requires both `start_date` and `end_date` parameters. The notebook has been updated to handle this correctly by determining the appropriate date range based on the selected mode.

## Requirements

Make sure you have all the required dependencies installed:

```bash
pip install -r ../requirements.txt
```

## Additional Resources

- For detailed information about the pipeline components, refer to the documentation in the `docs/` directory.
- Source code for the pipeline components is available in the `src/` directory.
- Configuration files are located in the `config/` directory. 