{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 01:37:40,509 - INFO - Validating data structure\n",
      "2025-04-03 01:37:40,510 - INFO - Data structure validation completed successfully\n",
      "2025-04-03 01:37:40,511 - INFO - Validating data structure\n",
      "2025-04-03 01:37:40,512 - INFO - Data structure validation completed successfully\n",
      "2025-04-03 01:37:40,530 - INFO - Generated run name: shap_t1_min10_max40_cumc95_20250403_1237_SYD\n",
      "2025-04-03 01:37:40,531 - INFO - Saved run configuration to /home/siddharth.johri/DECOHERE/data/results/feature_selection/run_shap_t1_min10_max40_cumc95_20250403_1237_SYD/run_config.json\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Setup and Initialization ---\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from datetime import datetime\n",
    "import pytz  # We'll need to install this\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path(os.getcwd()).parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.data.data_processor import DataProcessor\n",
    "from src.data.efficient_data_storage import EfficientDataStorage, DataType, DataStage\n",
    "from src.features.feature_selector import FeatureSelector\n",
    "from src.data.feature_generator import FeatureGenerator\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)\n",
    "\n",
    "# Load configuration\n",
    "config_path = os.path.join(project_root, 'config', 'config.yaml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Initialize classes\n",
    "processor = DataProcessor(config, logging.getLogger('Processor'))\n",
    "storage = EfficientDataStorage(config, logging.getLogger('Storage'))\n",
    "feature_generator = FeatureGenerator(config, logging.getLogger('FeatureGenerator'))\n",
    "\n",
    "# Initialize FeatureSelector with correct config structure\n",
    "feature_selection_config = {\n",
    "    'data': {\n",
    "        'features': {\n",
    "            'base_dir': config['data']['base_dir'],\n",
    "            'fundamental': config['data']['features']['fundamentals']\n",
    "        }\n",
    "    },\n",
    "    'feature_selection': config['feature_selection'],\n",
    "    'output': {\n",
    "        'results_dir': os.path.join(config['data']['base_dir'], 'results', 'feature_selection')\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate descriptive run name with Sydney time if no custom name provided\n",
    "def generate_run_name(config: dict) -> str:\n",
    "    \"\"\"Generate a descriptive run name with Sydney timestamp.\"\"\"\n",
    "    # Get Sydney time\n",
    "    sydney_tz = pytz.timezone('Australia/Sydney')\n",
    "    sydney_time = datetime.now(sydney_tz)\n",
    "    \n",
    "    # Extract feature selection parameters\n",
    "    method = config['feature_selection'].get('method', 'shap_threshold')[:4]\n",
    "    min_thresh = f\"t{int(config['feature_selection'].get('min_threshold', 0.01) * 100)}\"\n",
    "    min_feat = f\"min{config['feature_selection'].get('min_features', 10)}\"\n",
    "    max_feat = f\"max{config['feature_selection'].get('max_features', 40)}\"\n",
    "    cumul = \"cum\" if config['feature_selection'].get('use_cumulative', True) else \"nocum\"\n",
    "    cumul_thresh = f\"c{int(config['feature_selection'].get('cumulative_threshold', 0.95) * 100)}\" if cumul == \"cum\" else \"\"\n",
    "    \n",
    "    # Format timestamp\n",
    "    timestamp = sydney_time.strftime(\"%Y%m%d_%H%M_SYD\")\n",
    "    \n",
    "    # Combine into run name\n",
    "    run_name = f\"{method}_{min_thresh}_{min_feat}_{max_feat}_{cumul}{cumul_thresh}_{timestamp}\"\n",
    "    return run_name\n",
    "\n",
    "# You can specify a custom run_id here, or leave it as None for automatic generation\n",
    "run_id = None  # Change this to your desired run name if needed\n",
    "if run_id is None:\n",
    "    run_id = generate_run_name(feature_selection_config)\n",
    "    logging.info(f\"Generated run name: {run_id}\")\n",
    "\n",
    "feature_selector = FeatureSelector(feature_selection_config, run_id=run_id, logger=logging.getLogger('FeatureSelector'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline function defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Processing Function (with Enhanced Features) ---\n",
    "def run_pipeline_for_date(date_str: str, processor: DataProcessor, storage: EfficientDataStorage, \n",
    "                         feature_generator: Optional[FeatureGenerator], config: dict):\n",
    "    \"\"\"Runs the full data pipeline for a single date, including enhanced features if available.\"\"\"\n",
    "    \n",
    "    if not processor or not storage:\n",
    "        logging.error(f\"[{date_str}] Processor or Storage not initialized. Aborting.\")\n",
    "        return False, None, None, None\n",
    "\n",
    "    logging.info(f\"--- Starting Pipeline for Date: {date_str} ---\")\n",
    "    processed_file_path = None\n",
    "    pre_feature_file_path = None\n",
    "    enhanced_feature_file_path = None\n",
    "    success = False\n",
    "\n",
    "    try:\n",
    "        # 1. Load and process raw data\n",
    "        raw_data = processor.load_raw_data(date_str)\n",
    "        if raw_data.empty:\n",
    "            logging.warning(f\"[{date_str}] No raw data found. Skipping remaining steps.\")\n",
    "            return True, None, None, None\n",
    "\n",
    "        # 2. Process data\n",
    "        transformed_data = processor.transform_raw_data(raw_data)\n",
    "        filled_data = processor.fill_missing_values(transformed_data)\n",
    "\n",
    "        # 3. Store processed data\n",
    "        processed_file_path = storage.store_data(\n",
    "            df=filled_data, \n",
    "            data_type=DataType.FUNDAMENTALS,\n",
    "            stage=DataStage.PROCESSED, \n",
    "            date=date_str\n",
    "        )\n",
    "\n",
    "        # 4. Generate and store pre-feature data\n",
    "        pre_feature_df = storage.processed_data_feat_gen(filled_data)\n",
    "        if not pre_feature_df.empty:\n",
    "            pre_feature_file_path = storage.store_data(\n",
    "                df=pre_feature_df, \n",
    "                data_type=DataType.FUNDAMENTALS,\n",
    "                stage=DataStage.FEATURES, \n",
    "                date=date_str, \n",
    "                sub_type='pre_feature_set'\n",
    "            )\n",
    "\n",
    "            # 5. Generate enhanced features if generator available\n",
    "            if feature_generator:\n",
    "                # Get parameters from config\n",
    "                feature_cfg = config.get('features', {})\n",
    "                hist_w = feature_cfg.get('hist_window', 6)\n",
    "                fwd_w = feature_cfg.get('fwd_window', 6)\n",
    "                target_m = feature_cfg.get('target_metric', 'PE_RATIO_RATIO_SIGNED_LOG')\n",
    "                sector_map_rel_path = feature_cfg.get('sector_mapping_path', None)\n",
    "                sector_map_abs_path = os.path.join(project_root, sector_map_rel_path) if sector_map_rel_path else None\n",
    "                sector_levels = feature_cfg.get('sector_levels_to_include', ['sector_1'])\n",
    "                include_sectors = feature_cfg.get('include_sector_features', True)\n",
    "\n",
    "                enhanced_feature_df = feature_generator.generate_enhanced_features(\n",
    "                    df=pre_feature_df,\n",
    "                    hist_window=hist_w,\n",
    "                    fwd_window=fwd_w,\n",
    "                    target_metric=target_m,\n",
    "                    sector_mapping_path=sector_map_abs_path,\n",
    "                    sector_levels_to_include=sector_levels,\n",
    "                    include_sector_features=include_sectors\n",
    "                )\n",
    "                \n",
    "                if not enhanced_feature_df.empty:\n",
    "                    enhanced_feature_file_path = storage.store_data(\n",
    "                        df=enhanced_feature_df, \n",
    "                        data_type=DataType.FUNDAMENTALS,\n",
    "                        stage=DataStage.FEATURES, \n",
    "                        date=date_str, \n",
    "                        sub_type='enhanced_features'\n",
    "                    )\n",
    "            else:\n",
    "                logging.info(f\"[{date_str}] FeatureGenerator not available. Skipping enhanced features.\")\n",
    "\n",
    "        success = True\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[{date_str}] Pipeline error: {e}\", exc_info=True)\n",
    "        success = False\n",
    "\n",
    "    finally:\n",
    "        logging.info(f\"--- Finished Pipeline for Date: {date_str} (Success: {success}) ---\")\n",
    "\n",
    "    return success, processed_file_path, pre_feature_file_path, enhanced_feature_file_path\n",
    "\n",
    "print(\"Pipeline function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 01:46:20,400 - INFO - --- Starting Pipeline Run for Date: 2024-09-02 ---\n",
      "2025-04-03 01:46:20,401 - INFO - --- Starting Pipeline for Date: 2024-09-02 ---\n",
      "2025-04-03 01:46:20,402 - INFO - Loading raw data from /home/siddharth.johri/DECOHERE/data/raw/fundamentals/financials_2024_09.pq\n",
      "2025-04-03 01:46:20,421 - INFO - Filtering data for date: 2024-09-02\n",
      "2025-04-03 01:46:20,429 - INFO - Loaded raw data with shape: (5223, 34)\n",
      "2025-04-03 01:46:20,430 - INFO - Calculating periods for each ticker using COHERE logic\n",
      "2025-04-03 01:46:20,442 - INFO - Number of unique fiscal months per ID: fiscal_month\n",
      "1    457\n",
      "2     43\n",
      "Name: count, dtype: int64\n",
      "2025-04-03 01:46:20,443 - INFO - Calculating periods by ID using id column: 'ID'\n",
      "2025-04-03 01:46:22,311 - INFO - Period counts: {-6: np.int64(1), -5: np.int64(476), -4: np.int64(486), -3: np.int64(492), -2: np.int64(497), -1: np.int64(499), 0: np.int64(500), 1: np.int64(451), 2: np.int64(450), 3: np.int64(424), 4: np.int64(346), 5: np.int64(292)}\n",
      "2025-04-03 01:46:22,312 - INFO - Available periods: [-6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
      "/home/siddharth.johri/DECOHERE/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/home/siddharth.johri/DECOHERE/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "2025-04-03 01:46:27,984 - INFO - Filling missing values\n",
      "2025-04-03 01:46:28,625 - INFO - Storing fundamentals data for date: 2024-09-02 in processed stage\n",
      "2025-04-03 01:46:28,694 - INFO - Saved data to: /home/siddharth.johri/DECOHERE/data/processed/fundamentals/year=2024/month=09/data_2024-09-02.pq\n",
      "2025-04-03 01:46:28,695 - INFO - File size: 4254077 bytes\n",
      "2025-04-03 01:46:28,695 - INFO - Generating features from processed data\n",
      "2025-04-03 01:46:28,696 - INFO - Initial shape: (4914, 133)\n",
      "2025-04-03 01:46:28,700 - INFO - Selected 54 features\n",
      "2025-04-03 01:46:28,701 - INFO - Final shape: (4914, 54)\n",
      "2025-04-03 01:46:28,701 - INFO - Selected columns: ['ID', 'PIT_DATE', 'PERIOD', 'NET_INCOME_COEFF_OF_VAR', 'EBIT_COEFF_OF_VAR', 'EBITDA_COEFF_OF_VAR', 'SALES_COEFF_OF_VAR', 'NET_INCOME_RAW_SIGNED_LOG', 'NET_INCOME_RAW_SCALED_SALES_SIGNED_LOG', 'EBIT_RAW_SIGNED_LOG', 'EBIT_RAW_SCALED_SALES_SIGNED_LOG', 'EBITDA_RAW_SIGNED_LOG', 'EBITDA_RAW_SCALED_SALES_SIGNED_LOG', 'SALES_RAW_SIGNED_LOG', 'NET_OPERATING_ASSETS_RAW_SIGNED_LOG', 'NET_OPERATING_ASSETS_RAW_SCALED_SALES_SIGNED_LOG', 'INVENTORIES_RAW_SIGNED_LOG', 'INVENTORIES_RAW_SCALED_SALES_SIGNED_LOG', 'FREE_CASH_FLOW_RAW_SIGNED_LOG', 'FREE_CASH_FLOW_RAW_SCALED_SALES_SIGNED_LOG', 'DIVIDEND_RAW_SIGNED_LOG', 'DIVIDEND_RAW_SCALED_SALES_SIGNED_LOG', 'CAPEX_RAW_SIGNED_LOG', 'CAPEX_RAW_SCALED_SALES_SIGNED_LOG', 'DEPRECIATION_RAW_SIGNED_LOG', 'DEPRECIATION_RAW_SCALED_SALES_SIGNED_LOG', 'NET_INCOME_CSTAT_STD_RAW_SIGNED_LOG', 'NET_INCOME_CSTAT_STD_RAW_SCALED_SALES_SIGNED_LOG', 'EBIT_CSTAT_STD_RAW_SIGNED_LOG', 'EBIT_CSTAT_STD_RAW_SCALED_SALES_SIGNED_LOG', 'EBITDA_CSTAT_STD_RAW_SIGNED_LOG', 'EBITDA_CSTAT_STD_RAW_SCALED_SALES_SIGNED_LOG', 'SALES_CSTAT_STD_RAW_SIGNED_LOG', 'RETURN_COM_EQY_CSTAT_STD_RAW_SIGNED_LOG', 'INVENTORY_TURNOVER_CSTAT_STD_RAW_SIGNED_LOG', 'INTEREST_EXPENSE_TO_TOTAL_DEBT_RATIO_SIGNED_LOG', 'RETURN_ON_ASSETS_RATIO_SIGNED_LOG', 'RETURN_COM_EQY_RATIO_SIGNED_LOG', 'PE_RATIO_RATIO_SIGNED_LOG', 'PREV_PE_RATIO_RATIO_SIGNED_LOG', 'PX_TO_BOOK_RATIO_RATIO_SIGNED_LOG', 'PREV_PX_TO_BOOK_RATIO_RATIO_SIGNED_LOG', 'DEBT_TO_EQUITY_RATIO_RATIO_SIGNED_LOG', 'NET_DEBT_TO_EQUITY_RATIO_RATIO_SIGNED_LOG', 'CURRENT_RATIO_RATIO_SIGNED_LOG', 'OPERATING_MARGIN_RATIO_SIGNED_LOG', 'ASSET_TURNOVER_RATIO_SIGNED_LOG', 'INVENTORY_TURNOVER_RATIO_SIGNED_LOG', 'INTEREST_COVERAGE_RATIO_SIGNED_LOG', 'QUICK_RATIO_RATIO_SIGNED_LOG', 'NET_INCOME_COEFF_OF_VAR_RATIO_SIGNED_LOG', 'EBIT_COEFF_OF_VAR_RATIO_SIGNED_LOG', 'EBITDA_COEFF_OF_VAR_RATIO_SIGNED_LOG', 'SALES_COEFF_OF_VAR_RATIO_SIGNED_LOG']\n",
      "2025-04-03 01:46:28,702 - INFO - Storing fundamentals data for date: 2024-09-02 in features stage\n",
      "2025-04-03 01:46:28,731 - INFO - Saved data to: /home/siddharth.johri/DECOHERE/data/features/fundamentals/pre_feature_set/year=2024/month=09/data_2024-09-02.pq\n",
      "2025-04-03 01:46:28,732 - INFO - File size: 1781627 bytes\n",
      "2025-04-03 01:46:28,733 - INFO - Starting enhanced feature generation...\n",
      "2025-04-03 01:46:28,733 - INFO - Using period range: -6 to 6 (inclusive)\n",
      "2025-04-03 01:46:28,734 - INFO - Identified metric columns: 12 scaled sales, 18 ratios (excl. target), 9 stdevs.\n",
      "2025-04-03 01:46:28,738 - INFO - Processing 500 ID/PIT_DATE groups...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5127cd8fc69648629d8f0ceeb58b4800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing groups:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 01:47:51,883 - INFO - Identified 786 potential numerical feature columns generated.\n",
      "2025-04-03 01:47:51,883 - INFO - Sector features requested but no path provided. Skipping.\n",
      "2025-04-03 01:47:51,889 - INFO - Ranking 786 numerical features cross-sectionally (by PIT_DATE)...\n",
      "2025-04-03 01:47:52,196 - INFO - Numerical feature ranking complete.\n",
      "2025-04-03 01:47:52,197 - INFO - Merging target variable: PE_RATIO_RATIO_SIGNED_LOG\n",
      "2025-04-03 01:47:52,207 - INFO - Feature generation pipeline complete.\n",
      "2025-04-03 01:47:52,208 - WARNING - Target 'PE_RATIO_RATIO_SIGNED_LOG' has 9.8% missing values.\n",
      "2025-04-03 01:47:52,213 - INFO - Final DataFrame shape: (500, 789)\n",
      "2025-04-03 01:47:52,233 - INFO - Storing fundamentals data for date: 2024-09-02 in features stage\n",
      "2025-04-03 01:47:52,358 - INFO - Saved data to: /home/siddharth.johri/DECOHERE/data/features/fundamentals/enhanced_features/year=2024/month=09/data_2024-09-02.pq\n",
      "2025-04-03 01:47:52,359 - INFO - File size: 2577107 bytes\n",
      "2025-04-03 01:47:52,359 - INFO - --- Finished Pipeline for Date: 2024-09-02 (Success: True) ---\n",
      "2025-04-03 01:47:52,364 - INFO - --- Completed Pipeline Run for Date: 2024-09-02 ---\n",
      "2025-04-03 01:47:52,364 - INFO - --- Starting Pipeline Run for Date: 2024-09-03 ---\n",
      "2025-04-03 01:47:52,365 - INFO - --- Starting Pipeline for Date: 2024-09-03 ---\n",
      "2025-04-03 01:47:52,365 - INFO - Loading raw data from /home/siddharth.johri/DECOHERE/data/raw/fundamentals/financials_2024_09.pq\n",
      "2025-04-03 01:47:52,383 - INFO - Filtering data for date: 2024-09-03\n",
      "2025-04-03 01:47:52,393 - INFO - Loaded raw data with shape: (5223, 34)\n",
      "2025-04-03 01:47:52,393 - INFO - Calculating periods for each ticker using COHERE logic\n",
      "2025-04-03 01:47:52,404 - INFO - Number of unique fiscal months per ID: fiscal_month\n",
      "1    457\n",
      "2     43\n",
      "Name: count, dtype: int64\n",
      "2025-04-03 01:47:52,405 - INFO - Calculating periods by ID using id column: 'ID'\n",
      "2025-04-03 01:47:54,298 - INFO - Period counts: {-6: np.int64(1), -5: np.int64(476), -4: np.int64(486), -3: np.int64(492), -2: np.int64(497), -1: np.int64(499), 0: np.int64(500), 1: np.int64(451), 2: np.int64(450), 3: np.int64(424), 4: np.int64(346), 5: np.int64(292)}\n",
      "2025-04-03 01:47:54,299 - INFO - Available periods: [-6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "2025-04-03 01:48:00,227 - INFO - Filling missing values\n",
      "2025-04-03 01:48:00,247 - INFO - Storing fundamentals data for date: 2024-09-03 in processed stage\n",
      "2025-04-03 01:48:00,320 - INFO - Saved data to: /home/siddharth.johri/DECOHERE/data/processed/fundamentals/year=2024/month=09/data_2024-09-03.pq\n",
      "2025-04-03 01:48:00,321 - INFO - File size: 4512073 bytes\n",
      "2025-04-03 01:48:00,322 - INFO - Generating features from processed data\n",
      "2025-04-03 01:48:00,322 - INFO - Initial shape: (4914, 133)\n",
      "2025-04-03 01:48:00,325 - INFO - Selected 54 features\n",
      "2025-04-03 01:48:00,326 - INFO - Final shape: (4914, 54)\n",
      "2025-04-03 01:48:00,326 - INFO - Selected columns: ['ID', 'PIT_DATE', 'PERIOD', 'NET_INCOME_COEFF_OF_VAR', 'EBIT_COEFF_OF_VAR', 'EBITDA_COEFF_OF_VAR', 'SALES_COEFF_OF_VAR', 'NET_INCOME_RAW_SIGNED_LOG', 'NET_INCOME_RAW_SCALED_SALES_SIGNED_LOG', 'EBIT_RAW_SIGNED_LOG', 'EBIT_RAW_SCALED_SALES_SIGNED_LOG', 'EBITDA_RAW_SIGNED_LOG', 'EBITDA_RAW_SCALED_SALES_SIGNED_LOG', 'SALES_RAW_SIGNED_LOG', 'NET_OPERATING_ASSETS_RAW_SIGNED_LOG', 'NET_OPERATING_ASSETS_RAW_SCALED_SALES_SIGNED_LOG', 'INVENTORIES_RAW_SIGNED_LOG', 'INVENTORIES_RAW_SCALED_SALES_SIGNED_LOG', 'FREE_CASH_FLOW_RAW_SIGNED_LOG', 'FREE_CASH_FLOW_RAW_SCALED_SALES_SIGNED_LOG', 'DIVIDEND_RAW_SIGNED_LOG', 'DIVIDEND_RAW_SCALED_SALES_SIGNED_LOG', 'CAPEX_RAW_SIGNED_LOG', 'CAPEX_RAW_SCALED_SALES_SIGNED_LOG', 'DEPRECIATION_RAW_SIGNED_LOG', 'DEPRECIATION_RAW_SCALED_SALES_SIGNED_LOG', 'NET_INCOME_CSTAT_STD_RAW_SIGNED_LOG', 'NET_INCOME_CSTAT_STD_RAW_SCALED_SALES_SIGNED_LOG', 'EBIT_CSTAT_STD_RAW_SIGNED_LOG', 'EBIT_CSTAT_STD_RAW_SCALED_SALES_SIGNED_LOG', 'EBITDA_CSTAT_STD_RAW_SIGNED_LOG', 'EBITDA_CSTAT_STD_RAW_SCALED_SALES_SIGNED_LOG', 'SALES_CSTAT_STD_RAW_SIGNED_LOG', 'RETURN_COM_EQY_CSTAT_STD_RAW_SIGNED_LOG', 'INVENTORY_TURNOVER_CSTAT_STD_RAW_SIGNED_LOG', 'INTEREST_EXPENSE_TO_TOTAL_DEBT_RATIO_SIGNED_LOG', 'RETURN_ON_ASSETS_RATIO_SIGNED_LOG', 'RETURN_COM_EQY_RATIO_SIGNED_LOG', 'PE_RATIO_RATIO_SIGNED_LOG', 'PREV_PE_RATIO_RATIO_SIGNED_LOG', 'PX_TO_BOOK_RATIO_RATIO_SIGNED_LOG', 'PREV_PX_TO_BOOK_RATIO_RATIO_SIGNED_LOG', 'DEBT_TO_EQUITY_RATIO_RATIO_SIGNED_LOG', 'NET_DEBT_TO_EQUITY_RATIO_RATIO_SIGNED_LOG', 'CURRENT_RATIO_RATIO_SIGNED_LOG', 'OPERATING_MARGIN_RATIO_SIGNED_LOG', 'ASSET_TURNOVER_RATIO_SIGNED_LOG', 'INVENTORY_TURNOVER_RATIO_SIGNED_LOG', 'INTEREST_COVERAGE_RATIO_SIGNED_LOG', 'QUICK_RATIO_RATIO_SIGNED_LOG', 'NET_INCOME_COEFF_OF_VAR_RATIO_SIGNED_LOG', 'EBIT_COEFF_OF_VAR_RATIO_SIGNED_LOG', 'EBITDA_COEFF_OF_VAR_RATIO_SIGNED_LOG', 'SALES_COEFF_OF_VAR_RATIO_SIGNED_LOG']\n",
      "2025-04-03 01:48:00,327 - INFO - Storing fundamentals data for date: 2024-09-03 in features stage\n",
      "2025-04-03 01:48:00,356 - INFO - Saved data to: /home/siddharth.johri/DECOHERE/data/features/fundamentals/pre_feature_set/year=2024/month=09/data_2024-09-03.pq\n",
      "2025-04-03 01:48:00,357 - INFO - File size: 1866824 bytes\n",
      "2025-04-03 01:48:00,358 - INFO - Starting enhanced feature generation...\n",
      "2025-04-03 01:48:00,359 - INFO - Using period range: -6 to 6 (inclusive)\n",
      "2025-04-03 01:48:00,359 - INFO - Identified metric columns: 12 scaled sales, 18 ratios (excl. target), 9 stdevs.\n",
      "2025-04-03 01:48:00,362 - INFO - Processing 500 ID/PIT_DATE groups...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27a716049d94f69912790a92bf7d6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing groups:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 01:49:25,199 - INFO - Identified 786 potential numerical feature columns generated.\n",
      "2025-04-03 01:49:25,200 - INFO - Sector features requested but no path provided. Skipping.\n",
      "2025-04-03 01:49:25,205 - INFO - Ranking 786 numerical features cross-sectionally (by PIT_DATE)...\n",
      "2025-04-03 01:49:25,368 - INFO - Numerical feature ranking complete.\n",
      "2025-04-03 01:49:25,369 - INFO - Merging target variable: PE_RATIO_RATIO_SIGNED_LOG\n",
      "2025-04-03 01:49:25,377 - INFO - Feature generation pipeline complete.\n",
      "2025-04-03 01:49:25,378 - WARNING - Target 'PE_RATIO_RATIO_SIGNED_LOG' has 9.8% missing values.\n",
      "2025-04-03 01:49:25,382 - INFO - Final DataFrame shape: (500, 789)\n",
      "2025-04-03 01:49:25,401 - INFO - Storing fundamentals data for date: 2024-09-03 in features stage\n",
      "2025-04-03 01:49:25,547 - INFO - Saved data to: /home/siddharth.johri/DECOHERE/data/features/fundamentals/enhanced_features/year=2024/month=09/data_2024-09-03.pq\n",
      "2025-04-03 01:49:25,548 - INFO - File size: 2741297 bytes\n",
      "2025-04-03 01:49:25,548 - INFO - --- Finished Pipeline for Date: 2024-09-03 (Success: True) ---\n",
      "2025-04-03 01:49:25,552 - INFO - --- Completed Pipeline Run for Date: 2024-09-03 ---\n",
      "2025-04-03 01:49:25,553 - INFO - --- Starting Pipeline Run for Date: 2024-09-04 ---\n",
      "2025-04-03 01:49:25,554 - INFO - --- Starting Pipeline for Date: 2024-09-04 ---\n",
      "2025-04-03 01:49:25,554 - INFO - Loading raw data from /home/siddharth.johri/DECOHERE/data/raw/fundamentals/financials_2024_09.pq\n",
      "2025-04-03 01:49:25,575 - INFO - Filtering data for date: 2024-09-04\n",
      "2025-04-03 01:49:25,583 - INFO - Loaded raw data with shape: (5224, 34)\n",
      "2025-04-03 01:49:25,584 - INFO - Calculating periods for each ticker using COHERE logic\n",
      "2025-04-03 01:49:25,595 - INFO - Number of unique fiscal months per ID: fiscal_month\n",
      "1    457\n",
      "2     43\n",
      "Name: count, dtype: int64\n",
      "2025-04-03 01:49:25,596 - INFO - Calculating periods by ID using id column: 'ID'\n",
      "2025-04-03 01:49:27,628 - INFO - Period counts: {-6: np.int64(1), -5: np.int64(476), -4: np.int64(486), -3: np.int64(492), -2: np.int64(497), -1: np.int64(499), 0: np.int64(500), 1: np.int64(451), 2: np.int64(450), 3: np.int64(425), 4: np.int64(346), 5: np.int64(292)}\n",
      "2025-04-03 01:49:27,629 - INFO - Available periods: [-6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_suffix}'] = df[field]\n",
      "/home/siddharth.johri/DECOHERE/src/data/data_processor.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "2025-04-03 01:49:33,483 - INFO - Filling missing values\n",
      "2025-04-03 01:49:33,503 - INFO - Storing fundamentals data for date: 2024-09-04 in processed stage\n",
      "2025-04-03 01:49:33,575 - INFO - Saved data to: /home/siddharth.johri/DECOHERE/data/processed/fundamentals/year=2024/month=09/data_2024-09-04.pq\n",
      "2025-04-03 01:49:33,576 - INFO - File size: 4512638 bytes\n",
      "2025-04-03 01:49:33,577 - INFO - Generating features from processed data\n",
      "2025-04-03 01:49:33,578 - INFO - Initial shape: (4915, 133)\n",
      "2025-04-03 01:49:33,581 - INFO - Selected 54 features\n",
      "2025-04-03 01:49:33,582 - INFO - Final shape: (4915, 54)\n",
      "2025-04-03 01:49:33,582 - INFO - Selected columns: ['ID', 'PIT_DATE', 'PERIOD', 'NET_INCOME_COEFF_OF_VAR', 'EBIT_COEFF_OF_VAR', 'EBITDA_COEFF_OF_VAR', 'SALES_COEFF_OF_VAR', 'NET_INCOME_RAW_SIGNED_LOG', 'NET_INCOME_RAW_SCALED_SALES_SIGNED_LOG', 'EBIT_RAW_SIGNED_LOG', 'EBIT_RAW_SCALED_SALES_SIGNED_LOG', 'EBITDA_RAW_SIGNED_LOG', 'EBITDA_RAW_SCALED_SALES_SIGNED_LOG', 'SALES_RAW_SIGNED_LOG', 'NET_OPERATING_ASSETS_RAW_SIGNED_LOG', 'NET_OPERATING_ASSETS_RAW_SCALED_SALES_SIGNED_LOG', 'INVENTORIES_RAW_SIGNED_LOG', 'INVENTORIES_RAW_SCALED_SALES_SIGNED_LOG', 'FREE_CASH_FLOW_RAW_SIGNED_LOG', 'FREE_CASH_FLOW_RAW_SCALED_SALES_SIGNED_LOG', 'DIVIDEND_RAW_SIGNED_LOG', 'DIVIDEND_RAW_SCALED_SALES_SIGNED_LOG', 'CAPEX_RAW_SIGNED_LOG', 'CAPEX_RAW_SCALED_SALES_SIGNED_LOG', 'DEPRECIATION_RAW_SIGNED_LOG', 'DEPRECIATION_RAW_SCALED_SALES_SIGNED_LOG', 'NET_INCOME_CSTAT_STD_RAW_SIGNED_LOG', 'NET_INCOME_CSTAT_STD_RAW_SCALED_SALES_SIGNED_LOG', 'EBIT_CSTAT_STD_RAW_SIGNED_LOG', 'EBIT_CSTAT_STD_RAW_SCALED_SALES_SIGNED_LOG', 'EBITDA_CSTAT_STD_RAW_SIGNED_LOG', 'EBITDA_CSTAT_STD_RAW_SCALED_SALES_SIGNED_LOG', 'SALES_CSTAT_STD_RAW_SIGNED_LOG', 'RETURN_COM_EQY_CSTAT_STD_RAW_SIGNED_LOG', 'INVENTORY_TURNOVER_CSTAT_STD_RAW_SIGNED_LOG', 'INTEREST_EXPENSE_TO_TOTAL_DEBT_RATIO_SIGNED_LOG', 'RETURN_ON_ASSETS_RATIO_SIGNED_LOG', 'RETURN_COM_EQY_RATIO_SIGNED_LOG', 'PE_RATIO_RATIO_SIGNED_LOG', 'PREV_PE_RATIO_RATIO_SIGNED_LOG', 'PX_TO_BOOK_RATIO_RATIO_SIGNED_LOG', 'PREV_PX_TO_BOOK_RATIO_RATIO_SIGNED_LOG', 'DEBT_TO_EQUITY_RATIO_RATIO_SIGNED_LOG', 'NET_DEBT_TO_EQUITY_RATIO_RATIO_SIGNED_LOG', 'CURRENT_RATIO_RATIO_SIGNED_LOG', 'OPERATING_MARGIN_RATIO_SIGNED_LOG', 'ASSET_TURNOVER_RATIO_SIGNED_LOG', 'INVENTORY_TURNOVER_RATIO_SIGNED_LOG', 'INTEREST_COVERAGE_RATIO_SIGNED_LOG', 'QUICK_RATIO_RATIO_SIGNED_LOG', 'NET_INCOME_COEFF_OF_VAR_RATIO_SIGNED_LOG', 'EBIT_COEFF_OF_VAR_RATIO_SIGNED_LOG', 'EBITDA_COEFF_OF_VAR_RATIO_SIGNED_LOG', 'SALES_COEFF_OF_VAR_RATIO_SIGNED_LOG']\n",
      "2025-04-03 01:49:33,583 - INFO - Storing fundamentals data for date: 2024-09-04 in features stage\n",
      "2025-04-03 01:49:33,612 - INFO - Saved data to: /home/siddharth.johri/DECOHERE/data/features/fundamentals/pre_feature_set/year=2024/month=09/data_2024-09-04.pq\n",
      "2025-04-03 01:49:33,613 - INFO - File size: 1867077 bytes\n",
      "2025-04-03 01:49:33,614 - INFO - Starting enhanced feature generation...\n",
      "2025-04-03 01:49:33,614 - INFO - Using period range: -6 to 6 (inclusive)\n",
      "2025-04-03 01:49:33,615 - INFO - Identified metric columns: 12 scaled sales, 18 ratios (excl. target), 9 stdevs.\n",
      "2025-04-03 01:49:33,619 - INFO - Processing 500 ID/PIT_DATE groups...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583991172ec04c639adb30e4872b2c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing groups:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 01:50:56,831 - INFO - Identified 786 potential numerical feature columns generated.\n",
      "2025-04-03 01:50:56,832 - INFO - Sector features requested but no path provided. Skipping.\n",
      "2025-04-03 01:50:56,837 - INFO - Ranking 786 numerical features cross-sectionally (by PIT_DATE)...\n",
      "2025-04-03 01:50:57,151 - INFO - Numerical feature ranking complete.\n",
      "2025-04-03 01:50:57,152 - INFO - Merging target variable: PE_RATIO_RATIO_SIGNED_LOG\n",
      "2025-04-03 01:50:57,159 - INFO - Feature generation pipeline complete.\n",
      "2025-04-03 01:50:57,160 - WARNING - Target 'PE_RATIO_RATIO_SIGNED_LOG' has 9.8% missing values.\n",
      "2025-04-03 01:50:57,164 - INFO - Final DataFrame shape: (500, 789)\n",
      "2025-04-03 01:50:57,184 - INFO - Storing fundamentals data for date: 2024-09-04 in features stage\n",
      "2025-04-03 01:50:57,311 - INFO - Saved data to: /home/siddharth.johri/DECOHERE/data/features/fundamentals/enhanced_features/year=2024/month=09/data_2024-09-04.pq\n",
      "2025-04-03 01:50:57,312 - INFO - File size: 2756397 bytes\n",
      "2025-04-03 01:50:57,313 - INFO - --- Finished Pipeline for Date: 2024-09-04 (Success: True) ---\n",
      "2025-04-03 01:50:57,317 - INFO - --- Completed Pipeline Run for Date: 2024-09-04 ---\n",
      "2025-04-03 01:50:57,318 - INFO - [2024-09-02] Verified processed data file: /home/siddharth.johri/DECOHERE/data/processed/fundamentals/year=2024/month=09/data_2024-09-02.pq\n",
      "2025-04-03 01:50:57,318 - INFO - [2024-09-02] Verified pre-feature data file: /home/siddharth.johri/DECOHERE/data/features/fundamentals/pre_feature_set/year=2024/month=09/data_2024-09-02.pq\n",
      "2025-04-03 01:50:57,319 - INFO - [2024-09-02] Verified enhanced features file: /home/siddharth.johri/DECOHERE/data/features/fundamentals/enhanced_features/year=2024/month=09/data_2024-09-02.pq\n",
      "2025-04-03 01:50:57,319 - INFO - [2024-09-02] Verification successful\n",
      "2025-04-03 01:50:57,320 - INFO - [2024-09-03] Verified processed data file: /home/siddharth.johri/DECOHERE/data/processed/fundamentals/year=2024/month=09/data_2024-09-03.pq\n",
      "2025-04-03 01:50:57,320 - INFO - [2024-09-03] Verified pre-feature data file: /home/siddharth.johri/DECOHERE/data/features/fundamentals/pre_feature_set/year=2024/month=09/data_2024-09-03.pq\n",
      "2025-04-03 01:50:57,321 - INFO - [2024-09-03] Verified enhanced features file: /home/siddharth.johri/DECOHERE/data/features/fundamentals/enhanced_features/year=2024/month=09/data_2024-09-03.pq\n",
      "2025-04-03 01:50:57,322 - INFO - [2024-09-03] Verification successful\n",
      "2025-04-03 01:50:57,322 - INFO - [2024-09-04] Verified processed data file: /home/siddharth.johri/DECOHERE/data/processed/fundamentals/year=2024/month=09/data_2024-09-04.pq\n",
      "2025-04-03 01:50:57,323 - INFO - [2024-09-04] Verified pre-feature data file: /home/siddharth.johri/DECOHERE/data/features/fundamentals/pre_feature_set/year=2024/month=09/data_2024-09-04.pq\n",
      "2025-04-03 01:50:57,323 - INFO - [2024-09-04] Verified enhanced features file: /home/siddharth.johri/DECOHERE/data/features/fundamentals/enhanced_features/year=2024/month=09/data_2024-09-04.pq\n",
      "2025-04-03 01:50:57,324 - INFO - [2024-09-04] Verification successful\n",
      "2025-04-03 01:50:57,324 - INFO - All pipeline runs completed and verified successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline execution and verification complete.\n"
     ]
    }
   ],
   "source": [
    "# --- new Cell 3: Execution Loop and Verification ---\n",
    "dates_to_process = ['2024-09-02', '2024-09-03', '2024-09-04']\n",
    "results = {}\n",
    "\n",
    "# Process each date\n",
    "for date_str in dates_to_process:\n",
    "    logging.info(f\"--- Starting Pipeline Run for Date: {date_str} ---\")\n",
    "    success, proc_path, pre_feat_path, enh_feat_path = run_pipeline_for_date(\n",
    "        date_str=date_str,\n",
    "        processor=processor,\n",
    "        storage=storage,\n",
    "        feature_generator=feature_generator,\n",
    "        config=config\n",
    "    )\n",
    "    results[date_str] = (success, proc_path, pre_feat_path, enh_feat_path)\n",
    "    logging.info(f\"--- Completed Pipeline Run for Date: {date_str} ---\")\n",
    "\n",
    "# Verify results\n",
    "def verify_pipeline_output(date_str: str, result: tuple, expect_files: bool = True) -> bool:\n",
    "    \"\"\"Verify the pipeline output for a specific date.\"\"\"\n",
    "    success, proc_path, pre_feat_path, enh_feat_path = result\n",
    "    \n",
    "    if not success:\n",
    "        logging.error(f\"[{date_str}] Pipeline run failed\")\n",
    "        return False\n",
    "        \n",
    "    if expect_files:\n",
    "        # Check if all expected files exist\n",
    "        for path, file_type in [\n",
    "            (proc_path, \"processed data\"),\n",
    "            (pre_feat_path, \"pre-feature data\"),\n",
    "            (enh_feat_path, \"enhanced features\")\n",
    "        ]:\n",
    "            if path and not os.path.exists(path):\n",
    "                logging.error(f\"[{date_str}] Missing {file_type} file: {path}\")\n",
    "                return False\n",
    "            elif path:\n",
    "                logging.info(f\"[{date_str}] Verified {file_type} file: {path}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Verify all results\n",
    "all_success = True\n",
    "for date_str, result in results.items():\n",
    "    if not verify_pipeline_output(date_str, result):\n",
    "        all_success = False\n",
    "        logging.error(f\"[{date_str}] Verification failed\")\n",
    "    else:\n",
    "        logging.info(f\"[{date_str}] Verification successful\")\n",
    "\n",
    "if all_success:\n",
    "    logging.info(\"All pipeline runs completed and verified successfully\")\n",
    "else:\n",
    "    logging.error(\"Some pipeline runs failed verification\")\n",
    "\n",
    "print(\"Pipeline execution and verification complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison block\n",
    "\n",
    "# s =pd.read_parquet('/home/siddharth.johri/DECOHERE/data/raw/fundamentals/financials_2024_09.pq')\n",
    "# a = pd.read_parquet('/home/siddharth.johri/DECOHERE/data/processed/fundamentals/year=2024/month=09/data_2024-09-04.pq')\n",
    "# b = pd.read_parquet('/home/siddharth.johri/DECOHERE/data/features/fundamentals/pre_feature_set/year=2024/month=09/data_2024-09-04.pq')\n",
    "# c = pd.read_parquet('/home/siddharth.johri/DECOHERE/data/features/fundamentals/enhanced_features/year=2024/month=09/data_2024-09-04.pq')\n",
    "# a.query('ID == \"INFO IB Equity\" & PERIOD_END_DATE == \"2024-03-31\"')['PE_RATIO_RATIO']\n",
    "# c.query('ID == \"INFO IB Equity\"')['PE_RATIO_RATIO_SIGNED_LOG']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 03:35:35,762 - INFO - Starting feature selection for date: 2024-09-02\n",
      "2025-04-03 03:35:35,764 - ERROR - Data file not found: /home/siddharth.johri/DECOHERE/data/features/fundamentals/year=2024/month=09/data_2024-09-02.pq\n",
      "2025-04-03 03:35:35,765 - INFO - Analyzing feature stability from 2024-09-02 to 2024-09-04\n",
      "2025-04-03 03:35:35,766 - INFO - Saved feature stability analysis to /home/siddharth.johri/DECOHERE/data/results/feature_selection/run_shap_t1_min10_max40_cumc95_20250403_1237_SYD/feature_stability_shap_t1_min10_max40_cumc95_20250403_1237_SYD.csv\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m stability_results = feature_selector.analyze_feature_stability(start_date, end_date)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSelected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mselected_features\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcommon\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStability score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstability_results[\u001b[33m'\u001b[39m\u001b[33mstability_score\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTop 5 stable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstability_results[\u001b[33m'\u001b[39m\u001b[33mmost_stable_features\u001b[39m\u001b[33m'\u001b[39m][:\u001b[32m5\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# --- Cell 4: Feature Selection and Stability Analysis ---\n",
    "target_date = '2024-09-02'\n",
    "start_date, end_date = '2024-09-02', '2024-09-04'\n",
    "\n",
    "# Select features\n",
    "selected_features = feature_selector.select_features_daily(target_date)\n",
    "\n",
    "# Analyze stability\n",
    "stability_results = feature_selector.analyze_feature_stability(start_date, end_date)\n",
    "\n",
    "# Display results\n",
    "print(f\"Selected {len(selected_features['common'])} features for {target_date}\")\n",
    "print(f\"Stability score: {stability_results['stability_score']:.2%}\")\n",
    "print(f\"Top 5 stable: {stability_results['most_stable_features'][:5]}\")\n",
    "print(f\"Top 5 unstable: {stability_results['least_stable_features'][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_feature_selection(date: str, run_id: Optional[str] = None) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Run feature selection for a specific date.\n",
    "    \n",
    "    Args:\n",
    "        date: Date in YYYY-MM-DD format\n",
    "        run_id: Optional custom run identifier. If None, an intuitive name will be generated\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing selected features for each method\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Starting feature selection for date: {date}\")\n",
    "        \n",
    "        # Initialize feature selector\n",
    "        feature_selector = FeatureSelector(config, run_id=run_id, logger=logger)\n",
    "        \n",
    "        # Select features for the date\n",
    "        results = feature_selector.select_features_daily(date)\n",
    "        \n",
    "        if results:\n",
    "            logger.info(f\"Feature selection completed successfully for {date}\")\n",
    "            logger.info(f\"Selected features saved in run directory: {feature_selector.results_dir}\")\n",
    "        else:\n",
    "            logger.warning(f\"No features selected for {date}\")\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in feature selection for {date}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manalyze_feature_stability\u001b[39m(start_date: \u001b[38;5;28mstr\u001b[39m, end_date: \u001b[38;5;28mstr\u001b[39m, run_id: \u001b[38;5;28mstr\u001b[39m) -> \u001b[43mDict\u001b[49m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Analyze feature stability across a date range.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m \u001b[33;03m        Dictionary containing stability analysis results\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "def analyze_feature_stability(start_date: str, end_date: str, run_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze feature stability across a date range.\n",
    "    \n",
    "    Args:\n",
    "        start_date: Start date in YYYY-MM-DD format\n",
    "        end_date: End date in YYYY-MM-DD format\n",
    "        run_id: Run identifier to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing stability analysis results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Analyzing feature stability from {start_date} to {end_date}\")\n",
    "        \n",
    "        # Initialize feature selector with the same run_id\n",
    "        feature_selector = FeatureSelector(config, run_id=run_id, logger=logger)\n",
    "        \n",
    "        # Analyze stability\n",
    "        stability_metrics = feature_selector.analyze_feature_stability(start_date, end_date)\n",
    "        \n",
    "        logger.info(f\"Stability analysis completed. Results saved in run directory: {feature_selector.results_dir}\")\n",
    "        \n",
    "        return stability_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in stability analysis: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run feature selection for a specific date\n",
    "date = \"2024-03-15\"\n",
    "results = run_feature_selection(date)\n",
    "\n",
    "# Example: Run feature selection with custom run ID\n",
    "custom_run_id = \"my_experiment_1\"\n",
    "results = run_feature_selection(date, run_id=custom_run_id)\n",
    "\n",
    "# Example: Analyze feature stability\n",
    "start_date = \"2024-03-01\"\n",
    "end_date = \"2024-03-15\"\n",
    "stability = analyze_feature_stability(start_date, end_date, custom_run_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
