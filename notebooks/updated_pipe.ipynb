{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93597849",
   "metadata": {},
   "source": [
    "# DECOHERE Quantitative Trading Pipeline - Updated Version\n",
    "\n",
    "This notebook implements the updated pipeline structure with efficient data storage and processing.\n",
    "\n",
    "## Pipeline Components\n",
    "1. Data Loading and Storage\n",
    "2. Data Processing\n",
    "3. Feature Generation\n",
    "4. Feature Selection\n",
    "5. Model Training and Evaluation\n",
    "\n",
    "## Pipeline Modes\n",
    "- Day Mode: Process single day data\n",
    "- Week Mode: Process weekly data\n",
    "- Year Mode: Process yearly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d1a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path.cwd().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.data.efficient_data_storage import EfficientDataStorage, DataType, DataStage\n",
    "from src.data.data_processor import DataProcessor\n",
    "from src.features.feature_generator import FeatureGenerator\n",
    "from src.features.feature_selector import FeatureSelector\n",
    "from src.models.model_trainer import ModelTrainer\n",
    "from src.utils.logging_config import setup_logging\n",
    "from src.utils.config_loader import load_config, load_mode_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca1160",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb133d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 07:32:20,278 - DECOHERE - INFO - Pipeline initialization started\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'storage'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Initialize pipeline and make variables global\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m config, logger, storage, processor\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m config, logger, storage, processor = \u001b[43msetup_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36msetup_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mPipeline initialization started\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Initialize storage system\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m storage = \u001b[43mEfficientDataStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Initialize data processor\u001b[39;00m\n\u001b[32m     15\u001b[39m processor = DataProcessor(config, logger)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DECOHERE/src/data/efficient_data_storage.py:56\u001b[39m, in \u001b[36mEfficientDataStorage.__init__\u001b[39m\u001b[34m(self, config, logger)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Extract configuration parameters\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mself\u001b[39m.data_paths = config[\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28mself\u001b[39m.storage_config = \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprocessing\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstorage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Validate and create directory structure\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.validate_structure():\n",
      "\u001b[31mKeyError\u001b[39m: 'storage'"
     ]
    }
   ],
   "source": [
    "def setup_pipeline():\n",
    "    \"\"\"Initialize pipeline configuration and logging.\"\"\"\n",
    "    # Load configuration with absolute path\n",
    "    config_path = '/home/siddharth.johri/DECOHERE/config/config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    \n",
    "    # Setup logging\n",
    "    logger = setup_logging(config)\n",
    "    logger.info(\"Pipeline initialization started\")\n",
    "    \n",
    "    # Initialize storage system\n",
    "    storage = EfficientDataStorage(config, logger)\n",
    "    \n",
    "    # Initialize data processor\n",
    "    processor = DataProcessor(config, logger)\n",
    "    \n",
    "    logger.info(\"Pipeline initialization completed\")\n",
    "    return config, logger, storage, processor\n",
    "\n",
    "# Initialize pipeline and make variables global\n",
    "global config, logger, storage, processor\n",
    "config, logger, storage, processor = setup_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd673a8",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdc7d294",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m     55\u001b[39m date_str = datetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m processed_data, intermediate_data, returns_data = \u001b[43mload_and_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mday\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m processed_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessed data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessed_data.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mload_and_process_data\u001b[39m\u001b[34m(date_str, mode)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_and_process_data\u001b[39m(date_str: \u001b[38;5;28mstr\u001b[39m, mode: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33mday\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load and process data for a specific date and mode.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mlogger\u001b[49m.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading and processing data for date: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m mode\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      6\u001b[39m         \u001b[38;5;66;03m# Load fundamentals data\u001b[39;00m\n\u001b[32m      7\u001b[39m         fundamentals_df = storage.load_data(\n\u001b[32m      8\u001b[39m             data_type=DataType.FUNDAMENTALS,\n\u001b[32m      9\u001b[39m             stage=DataStage.RAW,\n\u001b[32m     10\u001b[39m             date=date_str,\n\u001b[32m     11\u001b[39m             mode=mode\n\u001b[32m     12\u001b[39m         )\n",
      "\u001b[31mNameError\u001b[39m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "def load_and_process_data(date_str: str, mode: str = 'day'):\n",
    "    \"\"\"Load and process data for a specific date and mode.\"\"\"\n",
    "    logger.info(f\"Loading and processing data for date: {date_str} in {mode} mode\")\n",
    "    \n",
    "    try:\n",
    "        # Load fundamentals data\n",
    "        fundamentals_df = storage.load_data(\n",
    "            data_type=DataType.FUNDAMENTALS,\n",
    "            stage=DataStage.RAW,\n",
    "            date=date_str,\n",
    "            mode=mode\n",
    "        )\n",
    "        \n",
    "        if fundamentals_df.empty:\n",
    "            logger.error(f\"No fundamentals data found for date: {date_str}\")\n",
    "            return None\n",
    "        \n",
    "        # Load returns data\n",
    "        returns_df = storage.load_data(\n",
    "            data_type=DataType.RETURNS,\n",
    "            stage=DataStage.RAW,\n",
    "            date=date_str,\n",
    "            mode=mode\n",
    "        )\n",
    "        \n",
    "        # Process fundamentals data - store intermediate results\n",
    "        intermediate_data = processor.process_fundamentals(fundamentals_df)\n",
    "        \n",
    "        # Store intermediate data\n",
    "        storage.store_data(\n",
    "            df=intermediate_data,\n",
    "            data_type=DataType.FUNDAMENTALS,\n",
    "            stage=DataStage.INTERMEDIATE,\n",
    "            date=date_str\n",
    "        )\n",
    "        \n",
    "        # Further process for final processed data\n",
    "        processed_fundamentals = processor.prepare_for_features(intermediate_data)\n",
    "        \n",
    "        # Store processed data\n",
    "        storage.store_data(\n",
    "            df=processed_fundamentals,\n",
    "            data_type=DataType.FUNDAMENTALS,\n",
    "            stage=DataStage.PROCESSED,\n",
    "            date=date_str\n",
    "        )\n",
    "        \n",
    "        return processed_fundamentals, intermediate_data, returns_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in data loading and processing: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "date_str = datetime.now().strftime('%Y-%m-%d')\n",
    "processed_data, intermediate_data, returns_data = load_and_process_data(date_str, mode='day')\n",
    "\n",
    "if processed_data is not None:\n",
    "    print(f\"Processed data shape: {processed_data.shape}\")\n",
    "    print(f\"Intermediate data shape: {intermediate_data.shape}\")\n",
    "    print(f\"Returns data shape: {returns_data.shape if not returns_data.empty else 'No returns data'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba90f2",
   "metadata": {},
   "source": [
    "## 3. Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b125f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(intermediate_data: pd.DataFrame, processed_data: pd.DataFrame, date_str: str):\n",
    "    \"\"\"Generate features from intermediate and processed data.\"\"\"\n",
    "    logger.info(f\"Generating features for date: {date_str}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize feature generator\n",
    "        feature_gen = FeatureGenerator(config, logger)\n",
    "        \n",
    "        # Generate features using both intermediate and processed data\n",
    "        features_df = feature_gen.generate_features(intermediate_data, processed_data)\n",
    "        \n",
    "        # Store features\n",
    "        storage.store_data(\n",
    "            df=features_df,\n",
    "            data_type=DataType.FUNDAMENTALS,\n",
    "            stage=DataStage.FEATURES,\n",
    "            date=date_str\n",
    "        )\n",
    "        \n",
    "        return features_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in feature generation: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate features\n",
    "if processed_data is not None and intermediate_data is not None:\n",
    "    features_df = generate_features(intermediate_data, processed_data, date_str)\n",
    "    if features_df is not None:\n",
    "        print(f\"Generated features shape: {features_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8bd524",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f27337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(features_df: pd.DataFrame, returns_df: pd.DataFrame):\n",
    "    \"\"\"Select the most relevant features.\"\"\"\n",
    "    logger.info(\"Starting feature selection\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize feature selector\n",
    "        selector = FeatureSelector(config, logger)\n",
    "        \n",
    "        # Select features\n",
    "        selected_features = selector.select_features(features_df, returns_df)\n",
    "        \n",
    "        return selected_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in feature selection: {e}\")\n",
    "        return None\n",
    "\n",
    "# Select features\n",
    "if features_df is not None and not returns_df.empty:\n",
    "    selected_features = select_features(features_df, returns_df)\n",
    "    if selected_features is not None:\n",
    "        print(f\"Selected {len(selected_features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9b3d28",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d08405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(features_df: pd.DataFrame, returns_df: pd.DataFrame, selected_features: list):\n",
    "    \"\"\"Train and evaluate the model.\"\"\"\n",
    "    logger.info(\"Starting model training and evaluation\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize model trainer\n",
    "        trainer = ModelTrainer(config, logger)\n",
    "        \n",
    "        # Train model\n",
    "        model = trainer.train_model(features_df, returns_df, selected_features)\n",
    "        \n",
    "        # Evaluate model\n",
    "        evaluation_results = trainer.evaluate_model(model, features_df, returns_df)\n",
    "        \n",
    "        return model, evaluation_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in model training and evaluation: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Train and evaluate model\n",
    "if selected_features is not None:\n",
    "    model, results = train_and_evaluate_model(features_df, returns_df, selected_features)\n",
    "    if model is not None and results is not None:\n",
    "        print(\"Model training and evaluation completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf5040e",
   "metadata": {},
   "source": [
    "## 6. Run Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(date_str: str, mode: str = 'day'):\n",
    "    \"\"\"Run the complete pipeline for a specific date and mode.\"\"\"\n",
    "    logger.info(f\"Starting pipeline run for date: {date_str} in {mode} mode\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load and process data\n",
    "        processed_data, returns_data = load_and_process_data(date_str, mode)\n",
    "        if processed_data is None:\n",
    "            return False\n",
    "        \n",
    "        # 2. Generate features\n",
    "        features_df = generate_features(processed_data, date_str)\n",
    "        if features_df is None:\n",
    "            return False\n",
    "        \n",
    "        # 3. Select features\n",
    "        selected_features = select_features(features_df, returns_data)\n",
    "        if selected_features is None:\n",
    "            return False\n",
    "        \n",
    "        # 4. Train and evaluate model\n",
    "        model, results = train_and_evaluate_model(features_df, returns_data, selected_features)\n",
    "        if model is None or results is None:\n",
    "            return False\n",
    "        \n",
    "        logger.info(\"Pipeline run completed successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in pipeline run: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run pipeline\n",
    "success = run_pipeline(date_str, mode='day')\n",
    "if success:\n",
    "    print(\"Pipeline completed successfully\")\n",
    "else:\n",
    "    print(\"Pipeline failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
