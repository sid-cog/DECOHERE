{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c857f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to the path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.data.data_processor import DataProcessor\n",
    "from src.features.feature_generator import FeatureGenerator\n",
    "from src.features.feature_selector import FeatureSelector\n",
    "from src.models.model_trainer import ModelTrainer\n",
    "from src.visualization.visualizer import Visualizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f10796",
   "metadata": {},
   "source": [
    "## Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd8082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "def setup_logging(config):\n",
    "    log_level = getattr(logging, config['logging']['level'])\n",
    "    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    \n",
    "    # Create logger\n",
    "    logger = logging.getLogger('decohere')\n",
    "    logger.setLevel(log_level)\n",
    "    \n",
    "    # Clear existing handlers\n",
    "    logger.handlers = []\n",
    "    \n",
    "    # Create console handler if enabled\n",
    "    if config['logging'].get('console', True):\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(log_level)\n",
    "        console_handler.setFormatter(logging.Formatter(log_format))\n",
    "        logger.addHandler(console_handler)\n",
    "    \n",
    "    # Create file handler if log file is specified\n",
    "    if 'file' in config['logging']:\n",
    "        log_dir = os.path.dirname(config['logging']['file'])\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        file_handler = logging.FileHandler(config['logging']['file'])\n",
    "        file_handler.setLevel(log_level)\n",
    "        file_handler.setFormatter(logging.Formatter(log_format))\n",
    "        logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f05976",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a2a6786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded main configuration from /home/siddharth.johri/DECOHERE/config/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Load the main configuration file\n",
    "def load_main_config():\n",
    "    config_path = '../config/config.yaml'\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "# Load mode-specific configuration\n",
    "def load_mode_config(mode):\n",
    "    main_config = load_main_config()\n",
    "    mode_config_path = main_config['modes'][mode]['config_file']\n",
    "    \n",
    "    # Convert relative path to absolute path\n",
    "    if not os.path.isabs(mode_config_path):\n",
    "        mode_config_path = os.path.join('..', mode_config_path)\n",
    "    \n",
    "    with open(mode_config_path, 'r') as f:\n",
    "        mode_config = yaml.safe_load(f)\n",
    "    \n",
    "    # Merge mode config with main config\n",
    "    merged_config = {**main_config, **mode_config}\n",
    "    return merged_config\n",
    "\n",
    "# Load the main configuration\n",
    "main_config = load_main_config()\n",
    "print(f\"Loaded main configuration from {os.path.abspath('../config/config.yaml')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8016f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.efficient_data_storage import EfficientDataStorage\n",
    "\n",
    "# Initialize data storage with config\n",
    "data_storage = EfficientDataStorage(config=main_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723bccdb",
   "metadata": {},
   "source": [
    "## Select Pipeline Mode\n",
    "\n",
    "Choose the mode to run the pipeline in:\n",
    "- **day**: Process a single day of data\n",
    "- **week**: Process a week of data\n",
    "- **year**: Process a year of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "720d3f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e352c59136d14b57846e6d0e32c80a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Mode:', options=('day', 'week', 'year'), value='day'), DatePicker(value=d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select the mode\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "mode_dropdown = widgets.Dropdown(\n",
    "    options=['day', 'week', 'year'],\n",
    "    value='day',\n",
    "    description='Mode:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "date_picker = widgets.DatePicker(\n",
    "    description='Date:',\n",
    "    disabled=False,\n",
    "    value=datetime.strptime('2024-09-02', '%Y-%m-%d').date()\n",
    "    #value= datetime.now().date()\n",
    ")\n",
    "\n",
    "display(widgets.VBox([mode_dropdown, date_picker]))\n",
    "\n",
    "# Function to get the selected mode and date\n",
    "def get_mode_and_date():\n",
    "    mode = mode_dropdown.value\n",
    "    date = date_picker.value\n",
    "    return mode, date\n",
    "\n",
    "# Load the configuration for the selected mode\n",
    "def load_selected_config():\n",
    "    mode, _ = get_mode_and_date()\n",
    "    config = load_mode_config(mode)\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf6df793",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tdf = pd.read_parquet('/home/siddharth.johri/DECOHERE/data/raw/financials/financials_2024_09.pq')\n",
    "# print(date_picker.value)\n",
    "# print(tdf.columns)\n",
    "# tdf[(tdf['pit_date']=='2024-09-02') & (tdf['ID']=='TATA IB Equity') ][['ID','PERIOD_END_DATE','pit_date','NET_INCOME']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f0620",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "Load raw financial data based on the selected mode and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d60ae44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 03:37:06,125 - decohere - INFO - Running pipeline in day mode for date 2024-09-02\n",
      "2025-03-31 03:37:06,127 - decohere - INFO - Loading raw data...\n",
      "2025-03-31 03:37:06,128 - decohere - INFO - Loading raw data from /home/siddharth.johri/DECOHERE/data/raw/financials/financials_2024_09.pq\n",
      "2025-03-31 03:37:06,165 - decohere - INFO - Filtering data for date: 2024-09-02\n",
      "2025-03-31 03:37:06,173 - decohere - INFO - Loaded raw data with shape: (5223, 34)\n",
      "2025-03-31 03:37:06,174 - decohere - INFO - Loaded 5223 rows of raw data\n"
     ]
    }
   ],
   "source": [
    "# Load the raw data\n",
    "def load_data():\n",
    "    # Get the selected mode and date\n",
    "    mode, date = get_mode_and_date()\n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Load the configuration for the selected mode\n",
    "    config = load_mode_config(mode)\n",
    "    \n",
    "    # Setup logging\n",
    "    logger = setup_logging(config)\n",
    "    logger.info(f\"Running pipeline in {mode} mode for date {date_str}\")\n",
    "    \n",
    "    # Initialize the data processor\n",
    "    data_processor = DataProcessor(config, logger)\n",
    "    \n",
    "    # Load the raw data\n",
    "    logger.info(\"Loading raw data...\")\n",
    "    raw_data = data_processor.load_raw_data(date=date_str)\n",
    "    logger.info(f\"Loaded {len(raw_data)} rows of raw data\")\n",
    "    \n",
    "    return raw_data, data_processor, config, logger\n",
    "\n",
    "# Execute data loading\n",
    "raw_data, data_processor, config, logger = load_data()\n",
    "\n",
    "# Display a sample of the raw data\n",
    "# display(raw_data.head())\n",
    "# print(f\"Raw data shape: {raw_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "792d9df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data[(raw_data['PIT_DATE']=='2024-09-02') & (raw_data['ID']=='ICICIBC IB Equity') ][['ID','PERIOD_END_DATE','PIT_DATE','NET_INCOME']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f80a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b6d4c90",
   "metadata": {},
   "source": [
    "## 2. Data Processing\n",
    "\n",
    "Clean and transform the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54434ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 03:37:06,468 - decohere - INFO - Processing raw data...\n",
      "2025-03-31 03:37:06,469 - decohere - INFO - Processing data from 2024-09-02 to 2024-09-02\n",
      "2025-03-31 03:37:06,471 - decohere - INFO - Processing data for date range: 2024-09-02 to 2024-09-02\n",
      "2025-03-31 03:37:06,472 - decohere - INFO - Processing data for date: 2024-09-02\n",
      "2025-03-31 03:37:06,472 - decohere - INFO - Loading raw data from /home/siddharth.johri/DECOHERE/data/raw/financials/financials_2024_09.pq\n",
      "2025-03-31 03:37:06,494 - decohere - INFO - Filtering data for date: 2024-09-02\n",
      "2025-03-31 03:37:06,502 - decohere - INFO - Loaded raw data with shape: (5223, 34)\n",
      "2025-03-31 03:37:06,503 - decohere - INFO - Calculating periods for each ticker using COHERE logic\n",
      "2025-03-31 03:37:06,516 - decohere - INFO - Number of unique fiscal months per ID: fiscal_month\n",
      "1    457\n",
      "2     43\n",
      "Name: count, dtype: int64\n",
      "2025-03-31 03:37:06,517 - decohere - INFO - Calculating periods by ID using id column: 'ID'\n",
      "2025-03-31 03:37:08,245 - decohere - INFO - Period counts: {-6.0: np.int64(1), -5.0: np.int64(476), -4.0: np.int64(486), -3.0: np.int64(492), -2.0: np.int64(497), -1.0: np.int64(499), 0.0: np.int64(500), 1.0: np.int64(451), 2.0: np.int64(450), 3.0: np.int64(424), 4.0: np.int64(346), 5.0: np.int64(292)}\n",
      "2025-03-31 03:37:08,247 - decohere - INFO - Available periods: [np.float64(-6.0), np.float64(-5.0), np.float64(-4.0), np.float64(-3.0), np.float64(-2.0), np.float64(-1.0), np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0)]\n",
      "/home/siddharth.johri/DECOHERE/notebooks/../src/data/data_processor.py:263: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{field}{self.ratio_signed_log_suffix}'] = self.signed_log_transform(df[field])\n",
      "/home/siddharth.johri/DECOHERE/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/home/siddharth.johri/DECOHERE/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/home/siddharth.johri/DECOHERE/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/home/siddharth.johri/DECOHERE/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/home/siddharth.johri/DECOHERE/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/home/siddharth.johri/DECOHERE/venv/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "2025-03-31 03:37:08,431 - decohere - INFO - Saved processed data using partitioned storage\n",
      "2025-03-31 03:37:08,432 - decohere - INFO - Processed 1 dates\n",
      "2025-03-31 03:37:08,433 - decohere - INFO - Loading processed data in day mode\n",
      "2025-03-31 03:37:08,446 - decohere - INFO - Processed data shape: (4914, 134)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PERIOD_END_DATE</th>\n",
       "      <th>NET_INCOME</th>\n",
       "      <th>NET_INCOME_CSTAT_STD</th>\n",
       "      <th>EBIT</th>\n",
       "      <th>EBIT_CSTAT_STD</th>\n",
       "      <th>EBITDA</th>\n",
       "      <th>EBITDA_CSTAT_STD</th>\n",
       "      <th>SALES</th>\n",
       "      <th>SALES_CSTAT_STD</th>\n",
       "      <th>...</th>\n",
       "      <th>SALES_COEFF_OF_VAR_RATIO_SIGNED_LOG</th>\n",
       "      <th>PE_RATIO_RATIO</th>\n",
       "      <th>PE_RATIO_RATIO_SIGNED_LOG</th>\n",
       "      <th>PREV_PE_RATIO_RATIO</th>\n",
       "      <th>PREV_PE_RATIO_RATIO_SIGNED_LOG</th>\n",
       "      <th>PX_TO_BOOK_RATIO_RATIO</th>\n",
       "      <th>PX_TO_BOOK_RATIO_RATIO_SIGNED_LOG</th>\n",
       "      <th>PREV_PX_TO_BOOK_RATIO_RATIO</th>\n",
       "      <th>PREV_PX_TO_BOOK_RATIO_RATIO_SIGNED_LOG</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2024-03-31</td>\n",
       "      <td>8.042100e+09</td>\n",
       "      <td>1.513435e+09</td>\n",
       "      <td>1.234330e+10</td>\n",
       "      <td>1.863053e+09</td>\n",
       "      <td>1.290970e+10</td>\n",
       "      <td>1.713381e+09</td>\n",
       "      <td>2.507030e+10</td>\n",
       "      <td>5.440580e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048624</td>\n",
       "      <td>47.922940</td>\n",
       "      <td>3.890246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.306098</td>\n",
       "      <td>2.510095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-09-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>6.579300e+09</td>\n",
       "      <td>1.513435e+09</td>\n",
       "      <td>1.160342e+10</td>\n",
       "      <td>1.863053e+09</td>\n",
       "      <td>1.370575e+10</td>\n",
       "      <td>1.713381e+09</td>\n",
       "      <td>1.565000e+10</td>\n",
       "      <td>5.440580e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048624</td>\n",
       "      <td>60.009417</td>\n",
       "      <td>4.111028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.396609</td>\n",
       "      <td>2.595002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-09-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>5.777385e+09</td>\n",
       "      <td>1.513435e+09</td>\n",
       "      <td>8.938944e+09</td>\n",
       "      <td>1.863053e+09</td>\n",
       "      <td>9.356376e+09</td>\n",
       "      <td>1.713381e+09</td>\n",
       "      <td>1.850650e+10</td>\n",
       "      <td>5.440580e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048624</td>\n",
       "      <td>294.117528</td>\n",
       "      <td>5.687374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.865364</td>\n",
       "      <td>2.629394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-09-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>1.561882e+09</td>\n",
       "      <td>1.513435e+09</td>\n",
       "      <td>6.072078e+09</td>\n",
       "      <td>1.863053e+09</td>\n",
       "      <td>6.293098e+09</td>\n",
       "      <td>1.713381e+09</td>\n",
       "      <td>9.706505e+09</td>\n",
       "      <td>5.440580e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048624</td>\n",
       "      <td>100.140303</td>\n",
       "      <td>4.616509</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.403186</td>\n",
       "      <td>2.667449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-09-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>2.011639e+09</td>\n",
       "      <td>1.513435e+09</td>\n",
       "      <td>1.160342e+10</td>\n",
       "      <td>1.863053e+09</td>\n",
       "      <td>1.370575e+10</td>\n",
       "      <td>1.713381e+09</td>\n",
       "      <td>7.377860e+10</td>\n",
       "      <td>5.440580e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048624</td>\n",
       "      <td>195.783173</td>\n",
       "      <td>5.282102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.283057</td>\n",
       "      <td>2.586489</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-09-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID PERIOD_END_DATE    NET_INCOME  NET_INCOME_CSTAT_STD  \\\n",
       "0  360ONE IB Equity      2024-03-31  8.042100e+09          1.513435e+09   \n",
       "1  360ONE IB Equity      2023-03-31  6.579300e+09          1.513435e+09   \n",
       "2  360ONE IB Equity      2022-03-31  5.777385e+09          1.513435e+09   \n",
       "3  360ONE IB Equity      2021-03-31  1.561882e+09          1.513435e+09   \n",
       "4  360ONE IB Equity      2020-03-31  2.011639e+09          1.513435e+09   \n",
       "\n",
       "           EBIT  EBIT_CSTAT_STD        EBITDA  EBITDA_CSTAT_STD         SALES  \\\n",
       "0  1.234330e+10    1.863053e+09  1.290970e+10      1.713381e+09  2.507030e+10   \n",
       "1  1.160342e+10    1.863053e+09  1.370575e+10      1.713381e+09  1.565000e+10   \n",
       "2  8.938944e+09    1.863053e+09  9.356376e+09      1.713381e+09  1.850650e+10   \n",
       "3  6.072078e+09    1.863053e+09  6.293098e+09      1.713381e+09  9.706505e+09   \n",
       "4  1.160342e+10    1.863053e+09  1.370575e+10      1.713381e+09  7.377860e+10   \n",
       "\n",
       "   SALES_CSTAT_STD  ...  SALES_COEFF_OF_VAR_RATIO_SIGNED_LOG  PE_RATIO_RATIO  \\\n",
       "0     5.440580e+09  ...                             0.048624       47.922940   \n",
       "1     5.440580e+09  ...                             0.048624       60.009417   \n",
       "2     5.440580e+09  ...                             0.048624      294.117528   \n",
       "3     5.440580e+09  ...                             0.048624      100.140303   \n",
       "4     5.440580e+09  ...                             0.048624      195.783173   \n",
       "\n",
       "   PE_RATIO_RATIO_SIGNED_LOG  PREV_PE_RATIO_RATIO  \\\n",
       "0                   3.890246                  NaN   \n",
       "1                   4.111028                  NaN   \n",
       "2                   5.687374                  NaN   \n",
       "3                   4.616509                  NaN   \n",
       "4                   5.282102                  NaN   \n",
       "\n",
       "   PREV_PE_RATIO_RATIO_SIGNED_LOG  PX_TO_BOOK_RATIO_RATIO  \\\n",
       "0                             NaN               11.306098   \n",
       "1                             NaN               12.396609   \n",
       "2                             NaN               12.865364   \n",
       "3                             NaN               13.403186   \n",
       "4                             NaN               12.283057   \n",
       "\n",
       "   PX_TO_BOOK_RATIO_RATIO_SIGNED_LOG  PREV_PX_TO_BOOK_RATIO_RATIO  \\\n",
       "0                           2.510095                          NaN   \n",
       "1                           2.595002                          NaN   \n",
       "2                           2.629394                          NaN   \n",
       "3                           2.667449                          NaN   \n",
       "4                           2.586489                          NaN   \n",
       "\n",
       "   PREV_PX_TO_BOOK_RATIO_RATIO_SIGNED_LOG        date  \n",
       "0                                     NaN  2024-09-02  \n",
       "1                                     NaN  2024-09-02  \n",
       "2                                     NaN  2024-09-02  \n",
       "3                                     NaN  2024-09-02  \n",
       "4                                     NaN  2024-09-02  \n",
       "\n",
       "[5 rows x 134 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data shape: (4914, 134)\n"
     ]
    }
   ],
   "source": [
    "# Process the raw data\n",
    "def process_data(raw_data, data_processor, logger):\n",
    "    \"\"\"\n",
    "    Process raw data and return processed data.\n",
    "    \n",
    "    Args:\n",
    "        raw_data: Raw data DataFrame\n",
    "        data_processor: DataProcessor instance\n",
    "        logger: Logger instance\n",
    "        \n",
    "    Returns:\n",
    "        Processed data DataFrame\n",
    "    \"\"\"\n",
    "    logger.info(\"Processing raw data...\")\n",
    "    \n",
    "    # Get the selected mode and date\n",
    "    mode, date = get_mode_and_date()\n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Determine date range based on mode\n",
    "    if mode == 'day':\n",
    "        start_date = date_str\n",
    "        end_date = date_str\n",
    "    elif mode == 'week':\n",
    "        start_date = date_str\n",
    "        end_date = (date + timedelta(days=6)).strftime('%Y-%m-%d')\n",
    "    elif mode == 'year':\n",
    "        start_date = date_str\n",
    "        end_date = (date + timedelta(days=364)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    logger.info(f\"Processing data from {start_date} to {end_date}\")\n",
    "    \n",
    "    # Process the data with date range\n",
    "    processed_files = data_processor.process_data(start_date, end_date)\n",
    "    \n",
    "    # Load the processed data using the data processor's load method\n",
    "    processed_data = data_processor.load_processed_data_by_mode(\n",
    "        mode=mode,\n",
    "        date=date_str if mode == 'day' else None,\n",
    "        start_date=start_date if mode in ['week', 'year'] else None,\n",
    "        end_date=end_date if mode in ['week', 'year'] else None\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Processed data shape: {processed_data.shape}\")\n",
    "    return processed_data\n",
    "\n",
    "# Execute data processing\n",
    "processed_data = process_data(raw_data, data_processor, logger)\n",
    "\n",
    "# Display a sample of the processed data\n",
    "display(processed_data.head())\n",
    "print(f\"Processed data shape: {processed_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "777d7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_data[(processed_data['PIT_DATE']=='2024-09-02') & (processed_data['ID']=='INFO IB Equity') ][['ID','PERIOD','PERIOD_END_DATE','PIT_DATE','NET_INCOME','NET_INCOME_CSTAT_STD']]\n",
    "#list(processed_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97a63e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 03:37:08,551 - decohere - INFO - Generating feature set from processed data\n",
      "2025-03-31 03:37:08,555 - decohere - INFO - Generated feature set with shape: (4914, 55)\n",
      "2025-03-31 03:37:08,556 - decohere - INFO - Number of features: 55\n",
      "2025-03-31 03:37:08,578 - decohere - INFO - Saved pre-feature set to /home/siddharth.johri/DECOHERE/notebooks/data/processed/pre_feature_data/pre_feature_set_2024-09-02.pq\n"
     ]
    }
   ],
   "source": [
    "# Generate and save feature set (automatically uses PIT_DATE)\n",
    "\n",
    "processed_data_feat_set = data_processor.processed_data_feat_gen(processed_data, scaling_field='SALES')\n",
    "#processed_data_feat_set.columns\n",
    "# # Load feature set (using either YYYY-MM or YYYY-MM-DD format)\n",
    "# loaded_feature_set = data_processor.load_pre_feature_set('2024-01')  # For monthly data\n",
    "# # or\n",
    "# loaded_feature_set = data_processor.load_pre_feature_set('2024-01-01')  # For daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa68c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet('/home/siddharth.johri/DECOHERE/data/raw/sector/sector.pq')\n",
    "# df1 = df.groupby(level='ID').first()\n",
    "# df1[df1.index == 'INFO IN Equity'][['sector_1','sector_2', 'sector_3', 'sector_4']]\n",
    "\n",
    "# df1.drop(columns=['Partial Errors'], inplace=True)\n",
    "# df1.to_parquet('/home/siddharth.johri/DECOHERE/data/raw/sector/sector_mappings.pq')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "795f754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input=processed_data_feat_set.copy(deep=True)\n",
    "sector_file_path = '/home/siddharth.johri/DECOHERE/data/raw/sector/sector_mappings.pq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6240946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking sample data for duplicates ---\n",
      "Checking sample data for global duplicates on ['ID', 'PIT_DATE', 'PERIOD']: Found 0\n",
      "\n",
      "--- Running feature generation WITH sector features ---\n",
      "Starting enhanced feature generation...\n",
      "Using period range: -2 to 4 (inclusive)\n",
      "Identified metric columns: 12 scaled sales, 18 ratios (excl. target), 9 stdevs.\n",
      "Processing 500 ID/PIT_DATE groups...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8aa1c4f1c84d95974f7c38a4650ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing groups:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 664 potential numerical feature columns generated.\n",
      "Loading sector mappings from: /home/siddharth.johri/DECOHERE/data/raw/sector/sector_mappings.pq\n",
      "Merging sector features for levels: ['sector_1', 'sector_2']\n",
      "Applying OneHotEncoding to: ['sector_1', 'sector_2']\n",
      "Added 33 OHE sector features (unranked).\n",
      "Ranking 664 numerical features cross-sectionally (by PIT_DATE)...\n",
      "Numerical feature ranking complete.\n",
      "Merging target variable: PE_RATIO_RATIO_SIGNED_LOG\n",
      "Feature generation pipeline complete.\n",
      "Warning: Target 'PE_RATIO_RATIO_SIGNED_LOG' has 9.8% missing values.\n",
      "Final DataFrame shape: (500, 700)\n",
      "\n",
      "--- Final DataFrame Schema (with sectors): ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Columns: 700 entries, ID to PE_RATIO_RATIO_SIGNED_LOG\n",
      "dtypes: datetime64[ns](1), float64(665), object(1), uint8(33)\n",
      "memory usage: 2.6 MB\n",
      "\n",
      "--- Final DataFrame Head (with sectors): ---\n",
      "                 ID   PIT_DATE  \\\n",
      "0  360ONE IB Equity 2024-09-02   \n",
      "1      3M IB Equity 2024-09-02   \n",
      "2    AACL IB Equity 2024-09-02   \n",
      "3   AAVAS IB Equity 2024-09-02   \n",
      "4     ABB IB Equity 2024-09-02   \n",
      "\n",
      "   rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_-1  \\\n",
      "0                                           0.501002      \n",
      "1                                           0.501002      \n",
      "2                                           0.501002      \n",
      "3                                           0.501002      \n",
      "4                                           0.501002      \n",
      "\n",
      "   rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_-2  \\\n",
      "0                                           0.501006      \n",
      "1                                           0.501006      \n",
      "2                                           0.501006      \n",
      "3                                           0.501006      \n",
      "4                                           0.501006      \n",
      "\n",
      "   rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_0  \\\n",
      "0                                              0.501     \n",
      "1                                              0.501     \n",
      "2                                              0.501     \n",
      "3                                              0.501     \n",
      "4                                              0.501     \n",
      "\n",
      "   rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_1  \\\n",
      "0                                           0.513304     \n",
      "1                                           0.513304     \n",
      "2                                           0.513304     \n",
      "3                                           0.022173     \n",
      "4                                           0.513304     \n",
      "\n",
      "   rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_2  \\\n",
      "0                                           0.493333     \n",
      "1                                           0.493333     \n",
      "2                                           0.493333     \n",
      "3                                           0.493333     \n",
      "4                                           0.493333     \n",
      "\n",
      "   rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_3  \\\n",
      "0                                           0.483491     \n",
      "1                                                NaN     \n",
      "2                                           0.483491     \n",
      "3                                           0.483491     \n",
      "4                                           0.483491     \n",
      "\n",
      "   rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_4  \\\n",
      "0                                           0.504335     \n",
      "1                                                NaN     \n",
      "2                                                NaN     \n",
      "3                                           0.504335     \n",
      "4                                           0.504335     \n",
      "\n",
      "   rank_as_is_CURRENT_RATIO_RATIO_SIGNED_LOG_period_-1  ...  \\\n",
      "0                                           0.551102    ...   \n",
      "1                                           0.899800    ...   \n",
      "2                                           0.717435    ...   \n",
      "3                                           0.551102    ...   \n",
      "4                                           0.633267    ...   \n",
      "\n",
      "   sector_2_Oil & Gas  sector_2_Real Estate  sector_2_Renewable Energy  \\\n",
      "0                   0                     0                          0   \n",
      "1                   0                     0                          0   \n",
      "2                   0                     0                          0   \n",
      "3                   0                     0                          0   \n",
      "4                   0                     0                          0   \n",
      "\n",
      "   sector_2_Retail & Wholesale - Staples  \\\n",
      "0                                      0   \n",
      "1                                      0   \n",
      "2                                      0   \n",
      "3                                      0   \n",
      "4                                      0   \n",
      "\n",
      "   sector_2_Retail & Whsle - Discretionary  sector_2_Software & Tech Services  \\\n",
      "0                                        0                                  0   \n",
      "1                                        0                                  0   \n",
      "2                                        0                                  0   \n",
      "3                                        0                                  0   \n",
      "4                                        0                                  0   \n",
      "\n",
      "   sector_2_Tech Hardware & Semiconductors  sector_2_Telecommunications  \\\n",
      "0                                        0                            0   \n",
      "1                                        0                            0   \n",
      "2                                        0                            0   \n",
      "3                                        0                            0   \n",
      "4                                        0                            0   \n",
      "\n",
      "   sector_2_Utilities  PE_RATIO_RATIO_SIGNED_LOG  \n",
      "0                   0                   3.714934  \n",
      "1                   0                   4.113432  \n",
      "2                   0                   3.841716  \n",
      "3                   0                   3.165248  \n",
      "4                   0                   4.512759  \n",
      "\n",
      "[5 rows x 700 columns]\n",
      "\n",
      "--- Verifying column categorization (with sectors): ---\n",
      "Total columns: 700\n",
      "ID/Target columns found: 3 (['ID', 'PIT_DATE', 'PE_RATIO_RATIO_SIGNED_LOG'])\n",
      "OHE columns found: 33\n",
      "Ranked columns found: 664\n",
      "Other columns (expected unranked numerical): 0\n",
      "Assertions passed: Column categories seem consistent.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os # For file operations in example\n",
    "# Optional: Import tqdm for progress bar\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except ImportError:\n",
    "    tqdm = None # Set tqdm to None if not installed\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def inv_signed_log(y: float) -> float:\n",
    "    \"\"\"Inverse of signed_log transformation: sign(y) * (exp(abs(y)) - 1).\"\"\"\n",
    "    if pd.isna(y):\n",
    "        return np.nan\n",
    "    try:\n",
    "        abs_y = np.abs(np.float64(y))\n",
    "        exp_val = np.exp(abs_y)\n",
    "        if np.isinf(exp_val):\n",
    "            return np.inf * np.sign(y)\n",
    "        return np.sign(y) * (exp_val - 1)\n",
    "    except OverflowError:\n",
    "        return np.inf * np.sign(y)\n",
    "\n",
    "\n",
    "def robust_slope(series: pd.Series, periods: list) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculate slope and R-squared robustly using linear regression.\n",
    "    Handles NaN, insufficient data (< 2 points), and constant data explicitly.\n",
    "    Uses the series index (expected to be period numbers) as the independent variable.\n",
    "    Returns: tuple[float, float]: (slope, r_squared). Returns (0.0, 0.0) for insufficient/constant data or errors.\n",
    "    \"\"\"\n",
    "    if not periods or series.empty:\n",
    "      return 0.0, 0.0\n",
    "\n",
    "    valid_indices = series.index.intersection(periods)\n",
    "    if valid_indices.empty:\n",
    "        return 0.0, 0.0\n",
    "    data = series.loc[valid_indices].dropna()\n",
    "\n",
    "    if len(data) < 2:\n",
    "        return 0.0, 0.0\n",
    "    if data.nunique() == 1:\n",
    "        return 0.0, 0.0 # Slope is 0, R^2 is ill-defined (treat as 0 fit)\n",
    "\n",
    "    x_values = data.index.astype(float)\n",
    "    y_values = data.values\n",
    "    try:\n",
    "        # Suppress RankWarning which can occur with few points but is handled by checks\n",
    "        # with warnings.catch_warnings():\n",
    "        #     warnings.filterwarnings('ignore', category=np.RankWarning)\n",
    "        slope, _, r_value, p_value, std_err = linregress(x_values, y_values)\n",
    "        if pd.isna(slope) or pd.isna(r_value):\n",
    "            return 0.0, 0.0\n",
    "        r_squared = r_value**2\n",
    "        return slope, r_squared\n",
    "    except ValueError as e:\n",
    "         print(f\"Warning: linregress failed unexpectedly for index {data.index.tolist()}: {e}. Returning (0.0, 0.0).\")\n",
    "         return 0.0, 0.0\n",
    "\n",
    "\n",
    "def calculate_stdev(series: pd.Series, periods: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculate sample standard deviation robustly for specified periods.\n",
    "    Ignores NaNs. Requires at least 2 data points. Uses ddof=1. Returns 0.0 for constant data.\n",
    "    Returns: float: Standard deviation, or np.nan if fewer than 2 data points.\n",
    "    \"\"\"\n",
    "    if not periods or series.empty:\n",
    "        return np.nan\n",
    "\n",
    "    valid_indices = series.index.intersection(periods)\n",
    "    if valid_indices.empty:\n",
    "        return np.nan\n",
    "    data = series.loc[valid_indices].dropna()\n",
    "\n",
    "    if len(data) < 2:\n",
    "        return np.nan\n",
    "    if data.nunique() == 1:\n",
    "        return 0.0 # Standard deviation of constant data is 0\n",
    "    return np.std(data.values, ddof=1)\n",
    "\n",
    "\n",
    "# --- Group Processing Function ---\n",
    "def process_group(group: pd.DataFrame, period_range: list, # period_range now includes 0\n",
    "                  raw_scaled_sales_signed_log_cols: list,\n",
    "                  ratio_signed_log_cols: list,\n",
    "                  cstat_std_cols: list) -> dict:\n",
    "    \"\"\"\n",
    "    Processes a single ID/PIT_DATE group to calculate time series features.\n",
    "    Historical periods (`hist_periods`) include PERIOD <= 0.\n",
    "    Forward periods (`fwd_periods`) include PERIOD > 0.\n",
    "    Handles duplicate PERIODs within a group by keeping the first occurrence and warning.\n",
    "    Returns: dict: A dictionary of calculated features for the group.\n",
    "    \"\"\"\n",
    "    # --- Initial Checks and Setup ---\n",
    "    if not all(col in group.columns for col in ['ID', 'PIT_DATE', 'PERIOD']):\n",
    "         return {}\n",
    "    if group.empty:\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        group_id = group['ID'].iloc[0]\n",
    "        group_pit_date = group['PIT_DATE'].iloc[0]\n",
    "    except IndexError:\n",
    "        return {}\n",
    "    feats = {'ID': group_id, 'PIT_DATE': group_pit_date}\n",
    "\n",
    "    # --- Handle duplicate PERIODs ---\n",
    "    if group['PERIOD'].duplicated().any():\n",
    "        n_dups = group['PERIOD'].duplicated().sum()\n",
    "        print(f\"Warning: Found {n_dups} duplicate PERIOD(s) in group ID {group_id}, PIT {group_pit_date}. Keeping first.\")\n",
    "        group = group.drop_duplicates(subset=['PERIOD'], keep='first').copy()\n",
    "\n",
    "    # --- Set Index ---\n",
    "    try:\n",
    "        group = group.set_index('PERIOD')\n",
    "        if not group.index.is_unique:\n",
    "             raise ValueError(\"Index not unique after dropping duplicates - unexpected.\")\n",
    "        if not group.index.is_monotonic_increasing:\n",
    "            group = group.sort_index()\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting index for group ID {group_id}, PIT {group_pit_date}: {e}. Skipping.\")\n",
    "        return {}\n",
    "\n",
    "    # Define index covering full range and period subsets\n",
    "    full_range_index = pd.Index(period_range, name='PERIOD') # Includes 0\n",
    "    # *** MODIFICATION: Historical periods include 0 ***\n",
    "    hist_periods = [p for p in period_range if p <= 0]\n",
    "    fwd_periods = [p for p in period_range if p > 0] # Forward remains > 0\n",
    "    combined_periods = period_range # Use the full range\n",
    "\n",
    "    numerical_feature_keys = []\n",
    "\n",
    "    # --- Process Metric Groups ---\n",
    "    for metric_group, metric_cols in [('scaled', raw_scaled_sales_signed_log_cols),\n",
    "                                      ('ratio', ratio_signed_log_cols)]:\n",
    "        for metric in metric_cols:\n",
    "            if metric not in group.columns:\n",
    "                continue\n",
    "\n",
    "            series = group[metric].reindex(full_range_index)\n",
    "\n",
    "            # Find specific period values (logic for latest negative is important)\n",
    "            negative_hist_periods = [p for p in hist_periods if p < 0] # Explicitly get negative periods\n",
    "            valid_negative_hist_indices = series.loc[series.index.intersection(negative_hist_periods)].dropna().index\n",
    "            valid_fwd_indices = series.loc[series.index.intersection(fwd_periods)].dropna().index\n",
    "\n",
    "            latest_negative_hist_period = valid_negative_hist_indices.max() if not valid_negative_hist_indices.empty else np.nan\n",
    "            first_fwd_period = valid_fwd_indices.min() if not valid_fwd_indices.empty else np.nan\n",
    "            value_period_0 = series.get(0, np.nan) # Get value at period 0\n",
    "\n",
    "            # Feature: Levels (Rename latest hist for clarity)\n",
    "            level_latest_neg_hist_key = f'level_latest_neg_hist_{metric}' # Renamed\n",
    "            level_period_0_key = f'level_period_0_{metric}'\n",
    "            level_first_fwd_key = f'level_first_fwd_{metric}'\n",
    "            feats[level_latest_neg_hist_key] = series.get(latest_negative_hist_period, np.nan) # Use specific variable\n",
    "            feats[level_period_0_key] = value_period_0\n",
    "            feats[level_first_fwd_key] = series.get(first_fwd_period, np.nan)\n",
    "            numerical_feature_keys.extend([level_latest_neg_hist_key, level_period_0_key, level_first_fwd_key])\n",
    "\n",
    "            # Features: Slopes and R-squared (hist slope now includes 0)\n",
    "            hist_slope, hist_r2 = robust_slope(series, hist_periods) # uses periods <= 0\n",
    "            fwd_slope, fwd_r2 = robust_slope(series, fwd_periods) # uses periods > 0\n",
    "            combined_slope, combined_r2 = robust_slope(series, combined_periods) # uses full range\n",
    "\n",
    "            hist_slope_key = f'{metric_group}_hist_slope_{metric}'\n",
    "            fwd_slope_key = f'{metric_group}_fwd_slope_{metric}'\n",
    "            combined_slope_key = f'{metric_group}_combined_slope_{metric}'\n",
    "            hist_r2_key = f'{metric_group}_hist_r2_{metric}'\n",
    "            fwd_r2_key = f'{metric_group}_fwd_r2_{metric}'\n",
    "            combined_r2_key = f'{metric_group}_combined_r2_{metric}'\n",
    "            feats.update({\n",
    "                hist_slope_key: hist_slope, fwd_slope_key: fwd_slope, combined_slope_key: combined_slope,\n",
    "                hist_r2_key: hist_r2, fwd_r2_key: fwd_r2, combined_r2_key: combined_r2\n",
    "            })\n",
    "            numerical_feature_keys.extend([\n",
    "                hist_slope_key, fwd_slope_key, combined_slope_key,\n",
    "                hist_r2_key, fwd_r2_key, combined_r2_key\n",
    "            ])\n",
    "\n",
    "            # Features: Volatility (hist vol now includes 0)\n",
    "            hist_vol = calculate_stdev(series, hist_periods) # uses periods <= 0\n",
    "            fwd_vol = calculate_stdev(series, fwd_periods) # uses periods > 0\n",
    "            combined_vol = calculate_stdev(series, combined_periods) # uses full range\n",
    "\n",
    "            hist_vol_key = f'{metric_group}_hist_vol_{metric}'\n",
    "            fwd_vol_key = f'{metric_group}_fwd_vol_{metric}'\n",
    "            combined_vol_key = f'{metric_group}_combined_vol_{metric}'\n",
    "            feats[hist_vol_key] = hist_vol\n",
    "            feats[fwd_vol_key] = fwd_vol\n",
    "            feats[combined_vol_key] = combined_vol\n",
    "            numerical_feature_keys.extend([hist_vol_key, fwd_vol_key, combined_vol_key])\n",
    "\n",
    "            # Features: Normalized Slopes (based on respective slope/vol calculations)\n",
    "            norm_hist_slope_key = f'{metric_group}_norm_hist_slope_{metric}'\n",
    "            norm_fwd_slope_key = f'{metric_group}_norm_fwd_slope_{metric}'\n",
    "            feats[norm_hist_slope_key] = hist_slope / hist_vol if pd.notna(hist_vol) and hist_vol != 0 else np.nan\n",
    "            feats[norm_fwd_slope_key] = fwd_slope / fwd_vol if pd.notna(fwd_vol) and fwd_vol != 0 else np.nan\n",
    "            numerical_feature_keys.extend([norm_hist_slope_key, norm_fwd_slope_key])\n",
    "\n",
    "            # Feature: Slope Divergence (hist slope now includes 0)\n",
    "            slope_divergence_key = f'{metric_group}_slope_divergence_{metric}'\n",
    "            feats[slope_divergence_key] = fwd_slope - hist_slope if pd.notna(fwd_slope) and pd.notna(hist_slope) else np.nan\n",
    "            numerical_feature_keys.append(slope_divergence_key)\n",
    "\n",
    "            # Feature: Acceleration (Slope of Differences)\n",
    "            diff_series = series.diff()\n",
    "            # Hist accel: slope of diffs for periods <= 0 (needs diff at index 0, -1, ...)\n",
    "            # Valid indices are those in hist_periods except the minimum period value\n",
    "            min_hist_period = min(hist_periods) if hist_periods else None\n",
    "            valid_hist_diff_periods = [\n",
    "                p for p in hist_periods\n",
    "                if p in diff_series.index and not pd.isna(diff_series.get(p)) and (min_hist_period is None or p > min_hist_period)\n",
    "            ]\n",
    "            # Fwd accel: slope of diffs for periods > 0 (needs diff at index 1, 2, ...)\n",
    "            valid_fwd_diff_periods = [p for p in fwd_periods if p in diff_series.index and not pd.isna(diff_series.get(p))]\n",
    "\n",
    "            hist_accel, _ = robust_slope(diff_series, valid_hist_diff_periods)\n",
    "            fwd_accel, _ = robust_slope(diff_series, valid_fwd_diff_periods)\n",
    "            hist_accel_key = f'{metric_group}_hist_accel_{metric}'\n",
    "            fwd_accel_key = f'{metric_group}_fwd_accel_{metric}'\n",
    "            feats[hist_accel_key] = hist_accel\n",
    "            feats[fwd_accel_key] = fwd_accel\n",
    "            numerical_feature_keys.extend([hist_accel_key, fwd_accel_key])\n",
    "\n",
    "    # --- Process Relative Dispersion (Still only for Forward Periods > 0) ---\n",
    "    for std_col in cstat_std_cols:\n",
    "        estimate_col = std_col.replace('_CSTAT_STD', '')\n",
    "        if not all(col in group.columns for col in [estimate_col, std_col]):\n",
    "            continue\n",
    "\n",
    "        std_series = group[std_col]\n",
    "        estimate_series = group[estimate_col]\n",
    "\n",
    "        # Loop only over positive forward periods for dispersion\n",
    "        for fwd_period in fwd_periods:\n",
    "            rel_disp_key = f'rel_disp_{std_col}_period_{fwd_period}'\n",
    "            numerical_feature_keys.append(rel_disp_key)\n",
    "            try:\n",
    "                slog_estimate = estimate_series.get(fwd_period)\n",
    "                slog_std = std_series.get(fwd_period)\n",
    "\n",
    "                if pd.isna(slog_estimate) or pd.isna(slog_std):\n",
    "                    feats[rel_disp_key] = np.nan\n",
    "                    continue\n",
    "\n",
    "                actual_estimate = inv_signed_log(slog_estimate)\n",
    "                actual_stdev = inv_signed_log(slog_std)\n",
    "\n",
    "                if pd.isna(actual_stdev) or actual_stdev < 0 or not np.isfinite(actual_stdev):\n",
    "                     feats[rel_disp_key] = np.nan\n",
    "                     continue\n",
    "                if pd.isna(actual_estimate) or not np.isfinite(actual_estimate):\n",
    "                     feats[rel_disp_key] = np.nan\n",
    "                     continue\n",
    "\n",
    "                denominator = max(abs(actual_estimate), 1e-9)\n",
    "                if denominator == 0:\n",
    "                    feats[rel_disp_key] = np.nan\n",
    "                    continue\n",
    "                relative_dispersion_log1p = np.log1p(actual_stdev / denominator)\n",
    "                feats[rel_disp_key] = relative_dispersion_log1p\n",
    "            except Exception as e:\n",
    "                feats[rel_disp_key] = np.nan\n",
    "                print(f'Error: Relative dispersion calc failed unexpectedly for ID {group_id}, key {rel_disp_key}: {e}')\n",
    "\n",
    "    # --- Process As-is Ratio Values (Includes PERIOD=0) ---\n",
    "    for period in period_range: # Loop includes 0\n",
    "        for ratio_metric in ratio_signed_log_cols:\n",
    "             as_is_key = f'as_is_{ratio_metric}_period_{period}'\n",
    "             metric_series = group.get(ratio_metric)\n",
    "             if metric_series is not None:\n",
    "                 feats[as_is_key] = metric_series.get(period, np.nan)\n",
    "             else:\n",
    "                 feats[as_is_key] = np.nan\n",
    "             numerical_feature_keys.append(as_is_key)\n",
    "\n",
    "    feats['_numerical_feature_keys'] = list(set(numerical_feature_keys))\n",
    "    return feats\n",
    "\n",
    "\n",
    "# --- Main Orchestration Function ---\n",
    "def generate_enhanced_features(\n",
    "    df: pd.DataFrame,\n",
    "    hist_window: int = 6,\n",
    "    fwd_window: int = 6,\n",
    "    target_metric: str = 'PE_RATIO_RATIO_SIGNED_LOG',\n",
    "    sector_mapping_path: str | None = None,\n",
    "    sector_levels_to_include: list = ['sector_1'],\n",
    "    include_sector_features: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates time series and optional sector features, including PERIOD=0 in hist/combined calculations.\n",
    "    \"\"\"\n",
    "    print(\"Starting enhanced feature generation...\")\n",
    "\n",
    "    # --- Input Validation and Setup ---\n",
    "    required_cols = ['ID', 'PIT_DATE', 'PERIOD']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        raise ValueError(f\"Input DataFrame is missing required columns: {missing}\")\n",
    "\n",
    "    if target_metric not in df.columns:\n",
    "         print(f\"Warning: Target metric '{target_metric}' not found in input DataFrame columns.\")\n",
    "\n",
    "    # *** MODIFICATION: Define period range INCLUDING 0 ***\n",
    "    if hist_window < 0 or fwd_window < 0:\n",
    "        raise ValueError(\"hist_window and fwd_window must be non-negative.\")\n",
    "    period_range = list(range(-hist_window, fwd_window + 1)) # Includes 0\n",
    "    print(f\"Using period range: {min(period_range)} to {max(period_range)} (inclusive)\")\n",
    "\n",
    "\n",
    "    # Identify metric columns dynamically\n",
    "    raw_scaled_sales_signed_log_cols = [col for col in df.columns if '_RAW_SCALED_SALES_SIGNED_LOG' in col]\n",
    "    ratio_signed_log_cols = [col for col in df.columns if '_RATIO_SIGNED_LOG' in col and col != target_metric]\n",
    "    cstat_std_cols = [col for col in df.columns if '_CSTAT_STD' in col]\n",
    "    print(f\"Identified metric columns: \"\n",
    "          f\"{len(raw_scaled_sales_signed_log_cols)} scaled sales, \"\n",
    "          f\"{len(ratio_signed_log_cols)} ratios (excl. target), \"\n",
    "          f\"{len(cstat_std_cols)} stdevs.\")\n",
    "\n",
    "\n",
    "    # --- Core Feature Generation (Group Apply) ---\n",
    "    global_duplicates = df.duplicated(subset=['ID', 'PIT_DATE', 'PERIOD']).sum()\n",
    "    if global_duplicates > 0:\n",
    "        print(f\"CRITICAL WARNING: Found {global_duplicates} duplicate rows in input based on (ID, PIT_DATE, PERIOD). Using first occurrence per group.\")\n",
    "\n",
    "    grouped = df.groupby(['ID', 'PIT_DATE'], observed=True, sort=False)\n",
    "    n_groups = grouped.ngroups\n",
    "    print(f\"Processing {n_groups} ID/PIT_DATE groups...\")\n",
    "\n",
    "    features_list = []\n",
    "    group_iterator = tqdm(grouped, total=n_groups, desc=\"Processing groups\") if tqdm else grouped\n",
    "\n",
    "    for group_key, group_data in group_iterator:\n",
    "        group_result = process_group(group_data.copy(), period_range, # Pass the new period_range\n",
    "                                     raw_scaled_sales_signed_log_cols,\n",
    "                                     ratio_signed_log_cols,\n",
    "                                     cstat_std_cols)\n",
    "        if group_result:\n",
    "            features_list.append(group_result)\n",
    "\n",
    "    if not tqdm: print(f\"Finished processing {n_groups} groups.\")\n",
    "\n",
    "    if not features_list:\n",
    "        print(\"Warning: No features generated. Returning empty DataFrame.\")\n",
    "        expected_cols = ['ID', 'PIT_DATE'] + ([target_metric] if target_metric in df.columns else [])\n",
    "        return pd.DataFrame(columns=expected_cols)\n",
    "\n",
    "    features_df = pd.DataFrame(features_list)\n",
    "\n",
    "    all_numerical_keys = set()\n",
    "    if '_numerical_feature_keys' in features_df.columns:\n",
    "        for keys_list in features_df['_numerical_feature_keys'].dropna():\n",
    "            if isinstance(keys_list, list):\n",
    "                 all_numerical_keys.update(keys_list)\n",
    "        features_df = features_df.drop(columns=['_numerical_feature_keys'])\n",
    "    numerical_feature_cols = [key for key in all_numerical_keys if key in features_df.columns]\n",
    "    print(f\"Identified {len(numerical_feature_cols)} potential numerical feature columns generated.\")\n",
    "\n",
    "\n",
    "    # --- Optional Sector Feature Integration (No changes needed here) ---\n",
    "    ohe_feature_names = []\n",
    "    if include_sector_features and sector_mapping_path:\n",
    "        try:\n",
    "            print(f\"Loading sector mappings from: {sector_mapping_path}\")\n",
    "            if not os.path.exists(sector_mapping_path):\n",
    "                raise FileNotFoundError(f\"Sector mapping file not found at {sector_mapping_path}\")\n",
    "\n",
    "            sector_df = pd.read_parquet(sector_mapping_path)\n",
    "            valid_sector_levels = [col for col in sector_levels_to_include if col in sector_df.columns]\n",
    "            if not valid_sector_levels:\n",
    "                 print(f\"Warning: None of specified sector levels found in {sector_mapping_path}. Skipping.\")\n",
    "            else:\n",
    "                cols_to_merge = ['ID'] + valid_sector_levels\n",
    "                sector_df = sector_df[cols_to_merge].drop_duplicates(subset=['ID'], keep='first')\n",
    "                print(f\"Merging sector features for levels: {valid_sector_levels}\")\n",
    "                original_feature_rows = len(features_df)\n",
    "                try:\n",
    "                    id_dtype_feat = features_df['ID'].dtype\n",
    "                    id_dtype_sect = sector_df['ID'].dtype\n",
    "                    if id_dtype_feat != id_dtype_sect:\n",
    "                         features_df['ID'] = features_df['ID'].astype(str)\n",
    "                         sector_df['ID'] = sector_df['ID'].astype(str)\n",
    "                    features_df = features_df.merge(sector_df, on='ID', how='left', validate='m:1')\n",
    "                    if len(features_df) != original_feature_rows:\n",
    "                         print(f\"Warning: Row count changed during sector merge.\")\n",
    "                except Exception as merge_err:\n",
    "                     print(f\"Error merging sectors: {merge_err}. Skipping.\")\n",
    "                     valid_sector_levels = []\n",
    "\n",
    "                sector_cols_in_features = [col for col in valid_sector_levels if col in features_df.columns]\n",
    "                if sector_cols_in_features:\n",
    "                    fill_value = 'Missing_Sector'\n",
    "                    features_df[sector_cols_in_features] = features_df[sector_cols_in_features].fillna(fill_value)\n",
    "                    print(f\"Applying OneHotEncoding to: {sector_cols_in_features}\")\n",
    "                    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore', dtype=np.uint8)\n",
    "                    encoded_sectors = ohe.fit_transform(features_df[sector_cols_in_features])\n",
    "                    ohe_feature_names = ohe.get_feature_names_out(sector_cols_in_features).tolist()\n",
    "                    encoded_sectors_df = pd.DataFrame(encoded_sectors, columns=ohe_feature_names, index=features_df.index)\n",
    "                    features_df = features_df.drop(columns=sector_cols_in_features)\n",
    "                    features_df = pd.concat([features_df, encoded_sectors_df], axis=1)\n",
    "                    print(f\"Added {len(ohe_feature_names)} OHE sector features (unranked).\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Warning: {e}. Skipping sector features.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing sector features: {e}. Skipping.\")\n",
    "            ohe_feature_names = []\n",
    "    elif include_sector_features and not sector_mapping_path:\n",
    "         print(\"Info: Sector features requested but no path provided. Skipping.\")\n",
    "\n",
    "\n",
    "    # --- Ranking (Only Numerical Features - No changes needed here) ---\n",
    "    processed_features_df = features_df.copy()\n",
    "    numerical_cols_to_rank_candidates = [col for col in numerical_feature_cols if col in processed_features_df.columns]\n",
    "    if not numerical_cols_to_rank_candidates:\n",
    "         print(\"Warning: No numerical feature columns found to rank.\")\n",
    "    else:\n",
    "        non_numeric_cols = processed_features_df[numerical_cols_to_rank_candidates].select_dtypes(exclude=[np.number]).columns\n",
    "        if non_numeric_cols.any():\n",
    "             print(f\"Warning: Non-numeric columns found among numerical candidates for ranking: {non_numeric_cols.tolist()}. Excluding.\")\n",
    "             numerical_cols_to_rank = [col for col in numerical_cols_to_rank_candidates if col not in non_numeric_cols]\n",
    "        else:\n",
    "             numerical_cols_to_rank = numerical_cols_to_rank_candidates\n",
    "\n",
    "        if not numerical_cols_to_rank:\n",
    "             print(\"Skipping ranking: No valid numeric columns remain.\")\n",
    "        else:\n",
    "            print(f\"Ranking {len(numerical_cols_to_rank)} numerical features cross-sectionally (by PIT_DATE)...\")\n",
    "            try:\n",
    "                ranked_data = processed_features_df.groupby('PIT_DATE')[numerical_cols_to_rank].transform(lambda x: x.rank(pct=True))\n",
    "                rename_dict = {col: f'rank_{col}' for col in numerical_cols_to_rank}\n",
    "                ranked_data = ranked_data.rename(columns=rename_dict)\n",
    "                processed_features_df = processed_features_df.drop(columns=numerical_cols_to_rank)\n",
    "                processed_features_df = pd.concat([processed_features_df, ranked_data], axis=1)\n",
    "                print(\"Numerical feature ranking complete.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during ranking: {e}. Proceeding with unranked features.\")\n",
    "\n",
    "\n",
    "    # --- Merge Target Variable (No changes needed here) ---\n",
    "    print(f\"Merging target variable: {target_metric}\")\n",
    "    if target_metric not in df.columns:\n",
    "         print(f\"Warning: Target metric '{target_metric}' not in original DataFrame. Cannot merge target.\")\n",
    "         if target_metric not in processed_features_df.columns:\n",
    "              processed_features_df[target_metric] = np.nan\n",
    "         final_df = processed_features_df\n",
    "    else:\n",
    "        target_df = df[df['PERIOD'] == 1][['ID', 'PIT_DATE', target_metric]].drop_duplicates(subset=['ID', 'PIT_DATE'], keep='first')\n",
    "        if target_df.empty:\n",
    "            print(f\"Warning: No data for PERIOD=1 to extract target '{target_metric}'. Target column will be all NaNs.\")\n",
    "            target_df_placeholder = processed_features_df[['ID', 'PIT_DATE']].drop_duplicates()\n",
    "            target_df_placeholder[target_metric] = np.nan\n",
    "            try:\n",
    "                final_df = processed_features_df.merge(target_df_placeholder, on=['ID', 'PIT_DATE'], how='left', validate='m:1')\n",
    "            except Exception as merge_err:\n",
    "                print(f\"Error merging placeholder target: {merge_err}.\")\n",
    "                if target_metric not in processed_features_df.columns: processed_features_df[target_metric] = np.nan\n",
    "                final_df = processed_features_df\n",
    "        else:\n",
    "            try:\n",
    "                id_dtype_feat = processed_features_df['ID'].dtype\n",
    "                id_dtype_target = target_df['ID'].dtype\n",
    "                if id_dtype_feat != id_dtype_target:\n",
    "                    processed_features_df['ID'] = processed_features_df['ID'].astype(str)\n",
    "                    target_df['ID'] = target_df['ID'].astype(str)\n",
    "                pit_dtype_feat = processed_features_df['PIT_DATE'].dtype\n",
    "                pit_dtype_target = target_df['PIT_DATE'].dtype\n",
    "                is_dt_feat = pd.api.types.is_datetime64_any_dtype(pit_dtype_feat)\n",
    "                is_dt_target = pd.api.types.is_datetime64_any_dtype(pit_dtype_target)\n",
    "                if is_dt_feat != is_dt_target or (is_dt_feat and pit_dtype_feat != pit_dtype_target):\n",
    "                    try:\n",
    "                        processed_features_df['PIT_DATE'] = pd.to_datetime(processed_features_df['PIT_DATE'])\n",
    "                        target_df['PIT_DATE'] = pd.to_datetime(target_df['PIT_DATE'])\n",
    "                    except Exception as date_err:\n",
    "                        print(f\"Error converting PIT_DATE types: {date_err}.\")\n",
    "                final_df = processed_features_df.merge(target_df, on=['ID', 'PIT_DATE'], how='left', validate='m:1')\n",
    "            except Exception as merge_err:\n",
    "                print(f\"Error merging target '{target_metric}': {merge_err}.\")\n",
    "                if target_metric not in processed_features_df.columns: processed_features_df[target_metric] = np.nan\n",
    "                final_df = processed_features_df\n",
    "\n",
    "    print(\"Feature generation pipeline complete.\")\n",
    "\n",
    "    # --- Final Checks and Cleanup (No changes needed here) ---\n",
    "    if final_df.empty and not features_df.empty:\n",
    "        print(\"Critical Warning: Final DataFrame is empty after merging target.\")\n",
    "        return final_df\n",
    "\n",
    "    if target_metric in final_df.columns:\n",
    "        missing_target_fraction = final_df[target_metric].isnull().mean()\n",
    "        if missing_target_fraction == 1.0: print(f\"Warning: Target '{target_metric}' is ALL missing.\")\n",
    "        elif missing_target_fraction > 0: print(f\"Warning: Target '{target_metric}' has {missing_target_fraction:.1%} missing values.\")\n",
    "    else:\n",
    "        print(f\"Warning: Target metric '{target_metric}' column not present in final DataFrame.\")\n",
    "\n",
    "    id_cols = ['ID', 'PIT_DATE']\n",
    "    feature_cols = [col for col in final_df.columns if col not in id_cols and col != target_metric]\n",
    "    final_cols_order = id_cols + sorted(feature_cols)\n",
    "    if target_metric in final_df.columns: final_cols_order.append(target_metric)\n",
    "    final_cols_order_exist = [col for col in final_cols_order if col in final_df.columns]\n",
    "    try:\n",
    "        final_df = final_df[final_cols_order_exist]\n",
    "    except KeyError as e:\n",
    "        print(f\"Warning: Could not reorder columns: {e}\")\n",
    "\n",
    "    print(f\"Final DataFrame shape: {final_df.shape}\")\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # --- Create Sample Data (Now includes PERIOD 0) ---\n",
    "    # print(\"\\n--- Setting up sample data (including duplicates and PERIOD 0) ---\")\n",
    "    # ids = ['A01'] * 8 + ['B02'] * 7 + ['C03'] * 6 # A01 has duplicate period 1\n",
    "    # pit_dates_list = pd.to_datetime(['2023-01-31'] * 8 + ['2023-01-31'] * 7 + ['2023-01-31'] * 6).tolist()\n",
    "    # # Sequence now includes 0\n",
    "    # periods_list = [-2, -1, 0, 1, 1, 2, 3, 4] + [-2, -1, 0, 1, 2, 3, 4] + [-2, -1, 0, 1, 2, 3] # C03 missing period 4\n",
    "\n",
    "    # df_input = pd.DataFrame({\n",
    "    #     'ID': ids,\n",
    "    #     'PIT_DATE': pd.to_datetime(pit_dates_list),\n",
    "    #     'PERIOD': periods_list,\n",
    "    #     'SALES_RAW_SCALED_SALES_SIGNED_LOG': np.random.randn(len(ids)) * 2,\n",
    "    #     'OTHER_RATIO_SIGNED_LOG': np.random.rand(len(ids)) - 0.5,\n",
    "    #     'PE_RATIO_RATIO_SIGNED_LOG': np.random.randn(len(ids)), # Target column name\n",
    "    #     'PE_RATIO_CSTAT_STD': np.abs(np.random.randn(len(ids)) * 0.1) + 0.05\n",
    "    # })\n",
    "\n",
    "    # # Assign specific target values at PERIOD=1 (affects first occurrence if duplicates)\n",
    "    # df_input.loc[(df_input['ID'] == 'A01') & (df_input['PERIOD'] == 1), 'PE_RATIO_RATIO_SIGNED_LOG'] = 10.5\n",
    "    # df_input.loc[(df_input['ID'] == 'B02') & (df_input['PERIOD'] == 1), 'PE_RATIO_RATIO_SIGNED_LOG'] = 15.0\n",
    "    # df_input.loc[(df_input['ID'] == 'C03') & (df_input['PERIOD'] == 1), 'PE_RATIO_RATIO_SIGNED_LOG'] = 8.2\n",
    "\n",
    "    # # --- Create dummy sector mapping file ---\n",
    "    # sector_file_path = './dummy_sector_mappings.pq'\n",
    "    # sector_data = pd.DataFrame({\n",
    "    #     'ID': ['A01', 'B02', 'C03', 'D04'],\n",
    "    #     'sector_1': ['Tech', 'Finance', 'Tech', 'Retail'],\n",
    "    #     'sector_2': ['Hardware', 'Banking', 'Software', 'Apparel']\n",
    "    # })\n",
    "    # try:\n",
    "    #     sector_data.to_parquet(sector_file_path)\n",
    "    #     print(f\"Created dummy sector file: {sector_file_path}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error creating dummy sector file: {e}\")\n",
    "\n",
    "    # --- Check for global duplicates in sample data ---\n",
    "    print(\"\\n--- Checking sample data for duplicates ---\")\n",
    "    key_cols_check = ['ID', 'PIT_DATE', 'PERIOD']\n",
    "    global_dup_check = df_input.duplicated(subset=key_cols_check).sum()\n",
    "    print(f\"Checking sample data for global duplicates on {key_cols_check}: Found {global_dup_check}\") # Should find 1\n",
    "\n",
    "    # --- Define target metric name for the example run ---\n",
    "    example_target_metric = 'PE_RATIO_RATIO_SIGNED_LOG' # Define variable here\n",
    "\n",
    "    # --- Run Feature Generation (With Sectors) ---\n",
    "    print(\"\\n--- Running feature generation WITH sector features ---\")\n",
    "    try:\n",
    "        final_features_with_sectors = generate_enhanced_features(\n",
    "            df_input.copy(),\n",
    "            hist_window=2,   # Max negative period is -2\n",
    "            fwd_window=4,    # Max positive period is 4\n",
    "            target_metric=example_target_metric,\n",
    "            sector_mapping_path=sector_file_path,\n",
    "            sector_levels_to_include=['sector_1', 'sector_2'], # Use existing levels\n",
    "            include_sector_features=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- ERROR during feature generation: {e} ---\")\n",
    "        final_features_with_sectors = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # --- Analyze Results (With Sectors) ---\n",
    "    print(\"\\n--- Final DataFrame Schema (with sectors): ---\")\n",
    "    if not final_features_with_sectors.empty:\n",
    "        final_features_with_sectors.info(verbose=False, memory_usage='deep')\n",
    "        print(\"\\n--- Final DataFrame Head (with sectors): ---\")\n",
    "        print(final_features_with_sectors.head())\n",
    "\n",
    "        # --- Assertions for Verification ---\n",
    "        print(\"\\n--- Verifying column categorization (with sectors): ---\")\n",
    "        all_cols = final_features_with_sectors.columns.tolist()\n",
    "        id_target_cols = ['ID', 'PIT_DATE', example_target_metric]\n",
    "        id_target_cols_present = [c for c in id_target_cols if c in all_cols]\n",
    "        ohe_cols = [col for col in all_cols if col.startswith('sector_')]\n",
    "        ranked_cols = [col for col in all_cols if col.startswith('rank_')]\n",
    "        # Find cols that don't fit known categories\n",
    "        other_cols = [c for c in all_cols if c not in id_target_cols_present and c not in ohe_cols and c not in ranked_cols]\n",
    "\n",
    "        print(f\"Total columns: {len(all_cols)}\")\n",
    "        print(f\"ID/Target columns found: {len(id_target_cols_present)} ({id_target_cols_present})\")\n",
    "        print(f\"OHE columns found: {len(ohe_cols)}\")\n",
    "        print(f\"Ranked columns found: {len(ranked_cols)}\")\n",
    "        print(f\"Other columns (expected unranked numerical): {len(other_cols)}\")\n",
    "\n",
    "        # Basic check: Ensure 'Other' columns don't start with prefixes of known categories\n",
    "        categories_ok = True\n",
    "        for col in other_cols:\n",
    "            if col.startswith('rank_') or col.startswith('sector_'):\n",
    "                print(f\"Assertion Error: Column '{col}' is not ranked/OHE but starts with prefix.\")\n",
    "                categories_ok = False\n",
    "            # Check if it looks like a generated feature that *should* have been ranked\n",
    "            # This check depends heavily on naming conventions used in process_group\n",
    "            if ('slope' in col or 'vol' in col or 'level' in col or 'r2' in col or 'accel' in col or 'rel_disp' in col or 'as_is' in col):\n",
    "                 # It's likely a numerical feature, it should ideally be ranked unless ranking failed\n",
    "                 pass # Allow unranked numerical if ranking failed, but ideally check warnings\n",
    "            # else: # If strict checking desired:\n",
    "            #     print(f\"Assertion Warning: Uncategorized column '{col}' found.\")\n",
    "            #     categories_ok = False\n",
    "\n",
    "        if categories_ok:\n",
    "             # Verify no overlap\n",
    "            all_categorized = set(id_target_cols_present) | set(ohe_cols) | set(ranked_cols) | set(other_cols)\n",
    "            if len(all_categorized) != len(all_cols):\n",
    "                 print(\"Assertion Error: Column overlap detected or uncategorized columns missed.\")\n",
    "                 categories_ok = False\n",
    "            else:\n",
    "                print(\"Assertions passed: Column categories seem consistent.\")\n",
    "        else:\n",
    "            print(\"Assertions failed: Check column naming and categorization.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Skipping checks as final DataFrame is empty.\")\n",
    "\n",
    "    # --- Clean up dummy file ---\n",
    "    # try:\n",
    "    #     if sector_file_path and os.path.exists(sector_file_path):\n",
    "    #         #  os.remove(sector_file_path)\n",
    "    #         #  print(f\"\\nRemoved dummy sector file: {sector_file_path}\")\n",
    "    # except OSError as e:\n",
    "    #     # print(f\"Error removing dummy file '{sector_file_path}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13a7a96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT_DATE</th>\n",
       "      <th>rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_-1</th>\n",
       "      <th>rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_-2</th>\n",
       "      <th>rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_0</th>\n",
       "      <th>rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_1</th>\n",
       "      <th>rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_2</th>\n",
       "      <th>rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_3</th>\n",
       "      <th>rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_4</th>\n",
       "      <th>rank_as_is_CURRENT_RATIO_RATIO_SIGNED_LOG_period_-1</th>\n",
       "      <th>...</th>\n",
       "      <th>sector_2_Oil &amp; Gas</th>\n",
       "      <th>sector_2_Real Estate</th>\n",
       "      <th>sector_2_Renewable Energy</th>\n",
       "      <th>sector_2_Retail &amp; Wholesale - Staples</th>\n",
       "      <th>sector_2_Retail &amp; Whsle - Discretionary</th>\n",
       "      <th>sector_2_Software &amp; Tech Services</th>\n",
       "      <th>sector_2_Tech Hardware &amp; Semiconductors</th>\n",
       "      <th>sector_2_Telecommunications</th>\n",
       "      <th>sector_2_Utilities</th>\n",
       "      <th>PE_RATIO_RATIO_SIGNED_LOG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>0.501002</td>\n",
       "      <td>0.501006</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.513304</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.483491</td>\n",
       "      <td>0.504335</td>\n",
       "      <td>0.551102</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.714934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3M IB Equity</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>0.501002</td>\n",
       "      <td>0.501006</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.513304</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.899800</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.113432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AACL IB Equity</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>0.501002</td>\n",
       "      <td>0.501006</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.513304</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.483491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.717435</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.841716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAVAS IB Equity</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>0.501002</td>\n",
       "      <td>0.501006</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.022173</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.483491</td>\n",
       "      <td>0.504335</td>\n",
       "      <td>0.551102</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.165248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABB IB Equity</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>0.501002</td>\n",
       "      <td>0.501006</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.513304</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.483491</td>\n",
       "      <td>0.504335</td>\n",
       "      <td>0.633267</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.512759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 700 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID   PIT_DATE  \\\n",
       "0  360ONE IB Equity 2024-09-02   \n",
       "1      3M IB Equity 2024-09-02   \n",
       "2    AACL IB Equity 2024-09-02   \n",
       "3   AAVAS IB Equity 2024-09-02   \n",
       "4     ABB IB Equity 2024-09-02   \n",
       "\n",
       "   rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_-1  \\\n",
       "0                                           0.501002      \n",
       "1                                           0.501002      \n",
       "2                                           0.501002      \n",
       "3                                           0.501002      \n",
       "4                                           0.501002      \n",
       "\n",
       "   rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_-2  \\\n",
       "0                                           0.501006      \n",
       "1                                           0.501006      \n",
       "2                                           0.501006      \n",
       "3                                           0.501006      \n",
       "4                                           0.501006      \n",
       "\n",
       "   rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_0  \\\n",
       "0                                              0.501     \n",
       "1                                              0.501     \n",
       "2                                              0.501     \n",
       "3                                              0.501     \n",
       "4                                              0.501     \n",
       "\n",
       "   rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_1  \\\n",
       "0                                           0.513304     \n",
       "1                                           0.513304     \n",
       "2                                           0.513304     \n",
       "3                                           0.022173     \n",
       "4                                           0.513304     \n",
       "\n",
       "   rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_2  \\\n",
       "0                                           0.493333     \n",
       "1                                           0.493333     \n",
       "2                                           0.493333     \n",
       "3                                           0.493333     \n",
       "4                                           0.493333     \n",
       "\n",
       "   rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_3  \\\n",
       "0                                           0.483491     \n",
       "1                                                NaN     \n",
       "2                                           0.483491     \n",
       "3                                           0.483491     \n",
       "4                                           0.483491     \n",
       "\n",
       "   rank_as_is_ASSET_TURNOVER_RATIO_SIGNED_LOG_period_4  \\\n",
       "0                                           0.504335     \n",
       "1                                                NaN     \n",
       "2                                                NaN     \n",
       "3                                           0.504335     \n",
       "4                                           0.504335     \n",
       "\n",
       "   rank_as_is_CURRENT_RATIO_RATIO_SIGNED_LOG_period_-1  ...  \\\n",
       "0                                           0.551102    ...   \n",
       "1                                           0.899800    ...   \n",
       "2                                           0.717435    ...   \n",
       "3                                           0.551102    ...   \n",
       "4                                           0.633267    ...   \n",
       "\n",
       "   sector_2_Oil & Gas  sector_2_Real Estate  sector_2_Renewable Energy  \\\n",
       "0                   0                     0                          0   \n",
       "1                   0                     0                          0   \n",
       "2                   0                     0                          0   \n",
       "3                   0                     0                          0   \n",
       "4                   0                     0                          0   \n",
       "\n",
       "   sector_2_Retail & Wholesale - Staples  \\\n",
       "0                                      0   \n",
       "1                                      0   \n",
       "2                                      0   \n",
       "3                                      0   \n",
       "4                                      0   \n",
       "\n",
       "   sector_2_Retail & Whsle - Discretionary  sector_2_Software & Tech Services  \\\n",
       "0                                        0                                  0   \n",
       "1                                        0                                  0   \n",
       "2                                        0                                  0   \n",
       "3                                        0                                  0   \n",
       "4                                        0                                  0   \n",
       "\n",
       "   sector_2_Tech Hardware & Semiconductors  sector_2_Telecommunications  \\\n",
       "0                                        0                            0   \n",
       "1                                        0                            0   \n",
       "2                                        0                            0   \n",
       "3                                        0                            0   \n",
       "4                                        0                            0   \n",
       "\n",
       "   sector_2_Utilities  PE_RATIO_RATIO_SIGNED_LOG  \n",
       "0                   0                   3.714934  \n",
       "1                   0                   4.113432  \n",
       "2                   0                   3.841716  \n",
       "3                   0                   3.165248  \n",
       "4                   0                   4.512759  \n",
       "\n",
       "[5 rows x 700 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_features_with_sectors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec556ee",
   "metadata": {},
   "source": [
    "## 3. Feature Generation\n",
    "\n",
    "Generate features for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1e20cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8db68ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_features_with_sectors.to_parquet('/home/siddharth.johri/DECOHERE/data/features/fundamental/test.pq')#['PE_RATIO_RATIO_SIGNED_LOG'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320022ff",
   "metadata": {},
   "source": [
    "## 4. Feature Selection\n",
    "\n",
    "Select important features using SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73c1f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "# def select_features(features, config, logger):\n",
    "#     logger.info(\"Selecting features...\")\n",
    "#     feature_selector = FeatureSelector(config, logger)\n",
    "    \n",
    "#     # Get target variables from config\n",
    "#     target_vars = config['features']['targets']\n",
    "    \n",
    "#     # Prepare X and y\n",
    "#     X = features.drop(columns=target_vars)\n",
    "#     y = features[target_vars[0]]  # Use the first target variable\n",
    "    \n",
    "#     # Select features\n",
    "#     selected_features = feature_selector.select_features(X, y)\n",
    "#     logger.info(f\"Selected {len(selected_features)} features\")\n",
    "    \n",
    "#     # Create dataset with selected features and target\n",
    "#     selected_data = features[selected_features + target_vars]\n",
    "    \n",
    "#     return selected_data, selected_features, feature_selector\n",
    "\n",
    "# # Execute feature selection\n",
    "# selected_data, selected_features, feature_selector = select_features(features, config, logger)\n",
    "\n",
    "# # Display the selected features\n",
    "# print(\"Selected features:\")\n",
    "# print(selected_features)\n",
    "# print(f\"\\nSelected data shape: {selected_data.shape}\")\n",
    "# display(selected_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c660718",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation\n",
    "\n",
    "Train and evaluate regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c287a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "# def train_and_evaluate_model(selected_data, config, logger):\n",
    "#     logger.info(\"Training and evaluating model...\")\n",
    "#     model_trainer = ModelTrainer(config, logger)\n",
    "    \n",
    "#     # Get target variables from config\n",
    "#     target_vars = config['features']['targets']\n",
    "    \n",
    "#     # Prepare X and y\n",
    "#     X = selected_data.drop(columns=target_vars)\n",
    "#     y = selected_data[target_vars[0]]  # Use the first target variable\n",
    "    \n",
    "#     # Train and evaluate model\n",
    "#     model, metrics = model_trainer.train_and_evaluate(X, y)\n",
    "    \n",
    "#     return model, metrics, model_trainer\n",
    "\n",
    "# # Execute model training and evaluation\n",
    "# model, metrics, model_trainer = train_and_evaluate_model(selected_data, config, logger)\n",
    "\n",
    "# # Display model metrics\n",
    "# print(\"Model Metrics:\")\n",
    "# for metric_name, metric_value in metrics.items():\n",
    "#     print(f\"{metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da9a97",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "Visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fce5463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "# def visualize_results(model, selected_data, selected_features, feature_selector, config, logger):\n",
    "#     logger.info(\"Visualizing results...\")\n",
    "#     visualizer = Visualizer(config, logger)\n",
    "    \n",
    "#     # Get target variables from config\n",
    "#     target_vars = config['features']['targets']\n",
    "    \n",
    "#     # Prepare X and y\n",
    "#     X = selected_data.drop(columns=target_vars)\n",
    "#     y = selected_data[target_vars[0]]  # Use the first target variable\n",
    "    \n",
    "#     # Plot feature importance\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     visualizer.plot_feature_importance(model, X.columns)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Plot SHAP values\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     visualizer.plot_shap_values(model, X)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Plot actual vs predicted values\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     y_pred = model.predict(X)\n",
    "#     visualizer.plot_actual_vs_predicted(y, y_pred)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     return visualizer\n",
    "\n",
    "# # Execute visualization\n",
    "# visualizer = visualize_results(model, selected_data, selected_features, feature_selector, config, logger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e7d746",
   "metadata": {},
   "source": [
    "## 7. Save Results\n",
    "\n",
    "Save the processed data, features, model, and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04868aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "# def save_results(processed_data, features, selected_data, model, metrics, config, logger):\n",
    "#     logger.info(\"Saving results...\")\n",
    "    \n",
    "#     # Get the selected mode and date\n",
    "#     mode, date = get_mode_and_date()\n",
    "#     date_str = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "#     # Create output directory\n",
    "#     output_dir = os.path.join('..', 'data', 'results', mode, date_str)\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     # Save processed data\n",
    "#     processed_data_path = os.path.join(output_dir, 'processed_data.csv')\n",
    "#     processed_data.to_csv(processed_data_path, index=False)\n",
    "#     logger.info(f\"Saved processed data to {processed_data_path}\")\n",
    "    \n",
    "#     # Save features\n",
    "#     features_path = os.path.join(output_dir, 'features.csv')\n",
    "#     features.to_csv(features_path, index=False)\n",
    "#     logger.info(f\"Saved features to {features_path}\")\n",
    "    \n",
    "#     # Save selected data\n",
    "#     selected_data_path = os.path.join(output_dir, 'selected_data.csv')\n",
    "#     selected_data.to_csv(selected_data_path, index=False)\n",
    "#     logger.info(f\"Saved selected data to {selected_data_path}\")\n",
    "    \n",
    "#     # Save model\n",
    "#     import joblib\n",
    "#     model_path = os.path.join(output_dir, 'model.joblib')\n",
    "#     joblib.dump(model, model_path)\n",
    "#     logger.info(f\"Saved model to {model_path}\")\n",
    "    \n",
    "#     # Save metrics\n",
    "#     import json\n",
    "#     metrics_path = os.path.join(output_dir, 'metrics.json')\n",
    "#     with open(metrics_path, 'w') as f:\n",
    "#         json.dump(metrics, f, indent=4)\n",
    "#     logger.info(f\"Saved metrics to {metrics_path}\")\n",
    "    \n",
    "#     return output_dir\n",
    "\n",
    "# # Execute saving results\n",
    "# output_dir = save_results(processed_data, features, selected_data, model, metrics, config, logger)\n",
    "# print(f\"Results saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf33aee",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "Display a summary of the pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cd65933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display summary\n",
    "# def display_summary(raw_data, processed_data, features, selected_data, selected_features, metrics, config):\n",
    "#     # Get the selected mode and date\n",
    "#     mode, date = get_mode_and_date()\n",
    "#     date_str = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(f\"DECOHERE Pipeline Summary - {mode.upper()} MODE - {date_str}\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     print(\"\\nData Processing:\")\n",
    "#     print(f\"  Raw data shape: {raw_data.shape}\")\n",
    "#     print(f\"  Processed data shape: {processed_data.shape}\")\n",
    "    \n",
    "#     print(\"\\nFeature Engineering:\")\n",
    "#     print(f\"  Total features generated: {features.shape[1] - len(config['features']['targets'])}\")\n",
    "#     print(f\"  Selected features: {len(selected_features)}\")\n",
    "    \n",
    "#     print(\"\\nModel Performance:\")\n",
    "#     for metric_name, metric_value in metrics.items():\n",
    "#         print(f\"  {metric_name}: {metric_value:.4f}\")\n",
    "    \n",
    "#     print(\"\\nTop 10 Important Features:\")\n",
    "#     for i, feature in enumerate(selected_features[:10]):\n",
    "#         print(f\"  {i+1}. {feature}\")\n",
    "    \n",
    "#     print(\"\\nResults saved to:\")\n",
    "#     print(f\"  {os.path.abspath(output_dir)}\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# # Execute summary display\n",
    "# display_summary(raw_data, processed_data, features, selected_data, selected_features, metrics, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5261cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
