{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2811fca7",
   "metadata": {},
   "source": [
    "# DECOHERE Pipeline\n",
    "\n",
    "This notebook provides an interface to run the DECOHERE quantitative trading pipeline. The pipeline consists of the following components:\n",
    "\n",
    "1. Data Loading: Load raw financial data\n",
    "2. Data Processing: Clean and transform the data\n",
    "3. Feature Generation: Create features for machine learning\n",
    "4. Feature Selection: Select important features using SHAP\n",
    "5. Regression: Train and evaluate regression models\n",
    "\n",
    "The pipeline can be run in three modes:\n",
    "- **Day Mode**: Process a single day of data\n",
    "- **Week Mode**: Process a week of data\n",
    "- **Year Mode**: Process a year of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c857f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to the path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.data.data_processor import DataProcessor\n",
    "from src.features.feature_generator import FeatureGenerator\n",
    "from src.features.feature_selector import FeatureSelector\n",
    "from src.models.model_trainer import ModelTrainer\n",
    "from src.visualization.visualizer import Visualizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f10796",
   "metadata": {},
   "source": [
    "## Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd8082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "def setup_logging(config):\n",
    "    log_level = getattr(logging, config['logging']['level'])\n",
    "    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    \n",
    "    # Create logger\n",
    "    logger = logging.getLogger('decohere')\n",
    "    logger.setLevel(log_level)\n",
    "    \n",
    "    # Clear existing handlers\n",
    "    logger.handlers = []\n",
    "    \n",
    "    # Create console handler if enabled\n",
    "    if config['logging'].get('console', True):\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(log_level)\n",
    "        console_handler.setFormatter(logging.Formatter(log_format))\n",
    "        logger.addHandler(console_handler)\n",
    "    \n",
    "    # Create file handler if log file is specified\n",
    "    if 'file' in config['logging']:\n",
    "        log_dir = os.path.dirname(config['logging']['file'])\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        file_handler = logging.FileHandler(config['logging']['file'])\n",
    "        file_handler.setLevel(log_level)\n",
    "        file_handler.setFormatter(logging.Formatter(log_format))\n",
    "        logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f05976",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a2a6786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded main configuration from /home/siddharth.johri/DECOHERE/config/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Load the main configuration file\n",
    "def load_main_config():\n",
    "    config_path = '../config/config.yaml'\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "# Load mode-specific configuration\n",
    "def load_mode_config(mode):\n",
    "    main_config = load_main_config()\n",
    "    mode_config_path = main_config['modes'][mode]['config_file']\n",
    "    \n",
    "    # Convert relative path to absolute path\n",
    "    if not os.path.isabs(mode_config_path):\n",
    "        mode_config_path = os.path.join('..', mode_config_path)\n",
    "    \n",
    "    with open(mode_config_path, 'r') as f:\n",
    "        mode_config = yaml.safe_load(f)\n",
    "    \n",
    "    # Merge mode config with main config\n",
    "    merged_config = {**main_config, **mode_config}\n",
    "    return merged_config\n",
    "\n",
    "# Load the main configuration\n",
    "main_config = load_main_config()\n",
    "print(f\"Loaded main configuration from {os.path.abspath('../config/config.yaml')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723bccdb",
   "metadata": {},
   "source": [
    "## Select Pipeline Mode\n",
    "\n",
    "Choose the mode to run the pipeline in:\n",
    "- **day**: Process a single day of data\n",
    "- **week**: Process a week of data\n",
    "- **year**: Process a year of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720d3f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4a151ebd1c4a26ac5334b63d066314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Mode:', options=('day', 'week', 'year'), value='day'), DatePicker(value=dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select the mode\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "mode_dropdown = widgets.Dropdown(\n",
    "    options=['day', 'week', 'year'],\n",
    "    value='day',\n",
    "    description='Mode:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "date_picker = widgets.DatePicker(\n",
    "    description='Date:',\n",
    "    disabled=False,\n",
    "    value=datetime.now().date()\n",
    ")\n",
    "\n",
    "display(widgets.VBox([mode_dropdown, date_picker]))\n",
    "\n",
    "# Function to get the selected mode and date\n",
    "def get_mode_and_date():\n",
    "    mode = mode_dropdown.value\n",
    "    date = date_picker.value\n",
    "    return mode, date\n",
    "\n",
    "# Load the configuration for the selected mode\n",
    "def load_selected_config():\n",
    "    mode, _ = get_mode_and_date()\n",
    "    config = load_mode_config(mode)\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6df793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 78336 entries, 0 to 78335\n",
      "Data columns (total 34 columns):\n",
      " #   Column                          Non-Null Count  Dtype         \n",
      "---  ------                          --------------  -----         \n",
      " 0   ID                              78336 non-null  object        \n",
      " 1   PERIOD_END_DATE                 78336 non-null  datetime64[ns]\n",
      " 2   NET_INCOME                      70546 non-null  float64       \n",
      " 3   NET_INCOME_cstat_std            20847 non-null  float64       \n",
      " 4   EBIT                            58504 non-null  float64       \n",
      " 5   EBIT_cstat_std                  19276 non-null  float64       \n",
      " 6   EBITDA                          55870 non-null  float64       \n",
      " 7   EBITDA_cstat_std                19577 non-null  float64       \n",
      " 8   SALES                           65314 non-null  float64       \n",
      " 9   SALES_cstat_std                 20922 non-null  float64       \n",
      " 10  INTEREST_EXPENSE_TO_TOTAL_DEBT  46229 non-null  float64       \n",
      " 11  RETURN_ON_ASSETS                60680 non-null  float64       \n",
      " 12  NET_OPERATING_ASSETS            43492 non-null  float64       \n",
      " 13  INVENTORIES                     44443 non-null  float64       \n",
      " 14  RETURN_COM_EQY                  67657 non-null  float64       \n",
      " 15  RETURN_COM_EQY_cstat_std        15717 non-null  float64       \n",
      " 16  PE_RATIO                        66056 non-null  float64       \n",
      " 17  PREV_PE_RATIO                   52842 non-null  float64       \n",
      " 18  PX_TO_BOOK_RATIO                65754 non-null  float64       \n",
      " 19  PREV_PX_TO_BOOK_RATIO           52603 non-null  float64       \n",
      " 20  DEBT_TO_EQUITY_RATIO            49297 non-null  float64       \n",
      " 21  NET_DEBT_TO_EQUITY_RATIO        62403 non-null  float64       \n",
      " 22  FREE_CASH_FLOW                  62067 non-null  float64       \n",
      " 23  CURRENT_RATIO                   46602 non-null  float64       \n",
      " 24  OPERATING_MARGIN                68685 non-null  float64       \n",
      " 25  ASSET_TURNOVER                  18501 non-null  float64       \n",
      " 26  DIVIDEND                        69372 non-null  float64       \n",
      " 27  INVENTORY_TURNOVER              70546 non-null  float64       \n",
      " 28  INVENTORY_TURNOVER_cstat_std    20847 non-null  float64       \n",
      " 29  INTEREST_COVERAGE               50928 non-null  float64       \n",
      " 30  QUICK_RATIO                     43902 non-null  float64       \n",
      " 31  CAPEX                           65192 non-null  float64       \n",
      " 32  DEPRECIATION                    54438 non-null  float64       \n",
      " 33  pit_date                        78336 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(31), object(2)\n",
      "memory usage: 20.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(78336, 34)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf = pd.read_parquet('/home/siddharth_johri/projects/data/financials/financials_2024_09.pq')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59f493b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PERIOD_END_DATE</th>\n",
       "      <th>pit_date</th>\n",
       "      <th>NET_INCOME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73113</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>3.745450e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73114</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>2.011639e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73115</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>1.561882e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73116</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>5.777385e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73117</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>6.579300e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73118</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2024-03-31</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>8.042100e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73119</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2025-03-31</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>9.842667e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73120</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2026-03-31</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>1.179256e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73121</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2027-03-31</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>1.366462e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73122</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2028-03-31</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>1.527200e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73123</th>\n",
       "      <td>360ONE IB Equity</td>\n",
       "      <td>2029-03-31</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>1.923200e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID PERIOD_END_DATE    pit_date    NET_INCOME\n",
       "73113  360ONE IB Equity      2019-03-31  2024-09-02  3.745450e+09\n",
       "73114  360ONE IB Equity      2020-03-31  2024-09-02  2.011639e+09\n",
       "73115  360ONE IB Equity      2021-03-31  2024-09-02  1.561882e+09\n",
       "73116  360ONE IB Equity      2022-03-31  2024-09-02  5.777385e+09\n",
       "73117  360ONE IB Equity      2023-03-31  2024-09-02  6.579300e+09\n",
       "73118  360ONE IB Equity      2024-03-31  2024-09-02  8.042100e+09\n",
       "73119  360ONE IB Equity      2025-03-31  2024-09-02  9.842667e+09\n",
       "73120  360ONE IB Equity      2026-03-31  2024-09-02  1.179256e+10\n",
       "73121  360ONE IB Equity      2027-03-31  2024-09-02  1.366462e+10\n",
       "73122  360ONE IB Equity      2028-03-31  2024-09-02  1.527200e+10\n",
       "73123  360ONE IB Equity      2029-03-31  2024-09-02  1.923200e+10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf[(tdf['pit_date']=='2024-09-02') & (tdf['ID']=='360ONE IB Equity') ][['ID','PERIOD_END_DATE','pit_date','NET_INCOME']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f0620",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "Load raw financial data based on the selected mode and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d60ae44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 05:30:50,569 - decohere - INFO - Running pipeline in day mode for date 2025-03-17\n",
      "2025-03-17 05:30:50,570 - decohere - INFO - Loading raw data...\n",
      "2025-03-17 05:30:50,570 - decohere - INFO - Loading raw data from /home/siddharth_johri/projects/data/financials/financials_2024_09.pq\n",
      "2025-03-17 05:30:50,600 - decohere - INFO - Filtering data for date: 2025-03-17\n",
      "2025-03-17 05:30:50,606 - decohere - INFO - Loaded raw data with shape: (0, 34)\n",
      "2025-03-17 05:30:50,606 - decohere - INFO - Loaded 0 rows of raw data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PERIOD_END_DATE</th>\n",
       "      <th>NET_INCOME</th>\n",
       "      <th>NET_INCOME_CSTAT_STD</th>\n",
       "      <th>EBIT</th>\n",
       "      <th>EBIT_CSTAT_STD</th>\n",
       "      <th>EBITDA</th>\n",
       "      <th>EBITDA_CSTAT_STD</th>\n",
       "      <th>SALES</th>\n",
       "      <th>SALES_CSTAT_STD</th>\n",
       "      <th>...</th>\n",
       "      <th>OPERATING_MARGIN</th>\n",
       "      <th>ASSET_TURNOVER</th>\n",
       "      <th>DIVIDEND</th>\n",
       "      <th>INVENTORY_TURNOVER</th>\n",
       "      <th>INVENTORY_TURNOVER_CSTAT_STD</th>\n",
       "      <th>INTEREST_COVERAGE</th>\n",
       "      <th>QUICK_RATIO</th>\n",
       "      <th>CAPEX</th>\n",
       "      <th>DEPRECIATION</th>\n",
       "      <th>PIT_DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ID, PERIOD_END_DATE, NET_INCOME, NET_INCOME_CSTAT_STD, EBIT, EBIT_CSTAT_STD, EBITDA, EBITDA_CSTAT_STD, SALES, SALES_CSTAT_STD, INTEREST_EXPENSE_TO_TOTAL_DEBT, RETURN_ON_ASSETS, NET_OPERATING_ASSETS, INVENTORIES, RETURN_COM_EQY, RETURN_COM_EQY_CSTAT_STD, PE_RATIO, PREV_PE_RATIO, PX_TO_BOOK_RATIO, PREV_PX_TO_BOOK_RATIO, DEBT_TO_EQUITY_RATIO, NET_DEBT_TO_EQUITY_RATIO, FREE_CASH_FLOW, CURRENT_RATIO, OPERATING_MARGIN, ASSET_TURNOVER, DIVIDEND, INVENTORY_TURNOVER, INVENTORY_TURNOVER_CSTAT_STD, INTEREST_COVERAGE, QUICK_RATIO, CAPEX, DEPRECIATION, PIT_DATE]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 34 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (0, 34)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw data\n",
    "def load_data():\n",
    "    # Get the selected mode and date\n",
    "    mode, date = get_mode_and_date()\n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Load the configuration for the selected mode\n",
    "    config = load_mode_config(mode)\n",
    "    \n",
    "    # Setup logging\n",
    "    logger = setup_logging(config)\n",
    "    logger.info(f\"Running pipeline in {mode} mode for date {date_str}\")\n",
    "    \n",
    "    # Initialize the data processor\n",
    "    data_processor = DataProcessor(config, logger)\n",
    "    \n",
    "    # Load the raw data\n",
    "    logger.info(\"Loading raw data...\")\n",
    "    raw_data = data_processor.load_raw_data(date=date_str)\n",
    "    logger.info(f\"Loaded {len(raw_data)} rows of raw data\")\n",
    "    \n",
    "    return raw_data, data_processor, config, logger\n",
    "\n",
    "# Execute data loading\n",
    "raw_data, data_processor, config, logger = load_data()\n",
    "\n",
    "# Display a sample of the raw data\n",
    "display(raw_data.head())\n",
    "print(f\"Raw data shape: {raw_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d4c90",
   "metadata": {},
   "source": [
    "## 2. Data Processing\n",
    "\n",
    "Clean and transform the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54434ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 01:37:49,043 - decohere - INFO - Processing raw data...\n",
      "2025-03-11 01:37:49,044 - decohere - INFO - Processing data from 2024-09-11 to 2024-09-11\n",
      "2025-03-11 01:37:49,044 - decohere - INFO - Processing data for date range: 2024-09-11 to 2024-09-11\n",
      "2025-03-11 01:37:49,046 - decohere - INFO - Processing data for date: 2024-09-11\n",
      "2025-03-11 01:37:49,046 - decohere - INFO - Loading raw data from /home/siddharth_johri/projects/data/financials/financials_2024_09.pq\n",
      "2025-03-11 01:37:49,060 - decohere - INFO - Filtering data for date: 2024-09-11\n",
      "2025-03-11 01:37:49,066 - decohere - INFO - Loaded raw data with shape: (5220, 34)\n",
      "2025-03-11 01:37:49,066 - decohere - INFO - Transforming raw data\n",
      "2025-03-11 01:37:49,075 - decohere - INFO - Calculating periods for each ticker\n",
      "2025-03-11 01:37:49,399 - decohere - INFO - Period counts: {0: 500, 1: 500, 2: 500, 3: 498, 4: 498, 5: 496, 6: 456, 7: 451, 8: 425, 9: 355, 10: 302, 11: 40, 12: 33, 13: 33, 14: 31, 15: 27, 16: 25, 17: 15, 18: 14, 19: 11, 20: 7, 21: 3}\n",
      "2025-03-11 01:37:49,400 - decohere - INFO - Filling missing values\n",
      "2025-03-11 01:37:49,400 - decohere - INFO - Filling missing values\n",
      "2025-03-11 01:37:53,435 - decohere - INFO - Winsorizing features with threshold: 3.0\n",
      "2025-03-11 01:37:53,483 - decohere - INFO - Transformed data with shape: (5220, 59)\n",
      "2025-03-11 01:37:53,506 - decohere - INFO - Saved processed data to data/processed/processed_2024-09-11.pq\n",
      "2025-03-11 01:37:53,507 - decohere - INFO - Processed 1 dates\n",
      "2025-03-11 01:37:53,508 - decohere - INFO - Loading processed data for 2024-09-11 from data/processed/processed_2024-09-11.pq\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x8a in position 7: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m processed_data\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Execute data processing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m processed_data = \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Display a sample of the processed data\u001b[39;00m\n\u001b[32m     39\u001b[39m display(processed_data.head())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mprocess_data\u001b[39m\u001b[34m(raw_data, data_processor, logger)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m date, file_path \u001b[38;5;129;01min\u001b[39;00m processed_files.items():\n\u001b[32m     28\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading processed data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     date_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     processed_data = pd.concat([processed_data, date_data], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     32\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessed data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessed_data.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DECOHERE/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DECOHERE/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DECOHERE/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DECOHERE/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DECOHERE/venv/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0x8a in position 7: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Process the raw data\n",
    "def process_data(raw_data, data_processor, logger):\n",
    "    logger.info(\"Processing raw data...\")\n",
    "    \n",
    "    # Get the selected mode and date\n",
    "    mode, date = get_mode_and_date()\n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Determine date range based on mode\n",
    "    if mode == 'day':\n",
    "        start_date = date_str\n",
    "        end_date = date_str\n",
    "    elif mode == 'week':\n",
    "        start_date = date_str\n",
    "        end_date = (date + timedelta(days=6)).strftime('%Y-%m-%d')\n",
    "    elif mode == 'year':\n",
    "        start_date = date_str\n",
    "        end_date = (date + timedelta(days=364)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    logger.info(f\"Processing data from {start_date} to {end_date}\")\n",
    "    \n",
    "    # Process the data with date range\n",
    "    processed_files = data_processor.process_data(start_date, end_date)\n",
    "    \n",
    "    # Load the processed data\n",
    "    processed_data = pd.DataFrame()\n",
    "    for date, file_path in processed_files.items():\n",
    "        logger.info(f\"Loading processed data for {date} from {file_path}\")\n",
    "        \n",
    "        # Read the file based on its extension\n",
    "        if file_path.endswith('.parquet') or file_path.endswith('.pq'):\n",
    "            logger.info(f\"Reading parquet file: {file_path}\")\n",
    "            date_data = pd.read_parquet(file_path)\n",
    "        elif file_path.endswith('.csv'):\n",
    "            logger.info(f\"Reading CSV file: {file_path}\")\n",
    "            date_data = pd.read_csv(file_path)\n",
    "        elif file_path.endswith('.xlsx') or file_path.endswith('.xls'):\n",
    "            logger.info(f\"Reading Excel file: {file_path}\")\n",
    "            date_data = pd.read_excel(file_path)\n",
    "        else:\n",
    "            logger.error(f\"Unsupported file format: {file_path}\")\n",
    "            raise ValueError(f\"Unsupported file format: {file_path}\")\n",
    "        \n",
    "        processed_data = pd.concat([processed_data, date_data], ignore_index=True)\n",
    "    \n",
    "    logger.info(f\"Processed data shape: {processed_data.shape}\")\n",
    "    return processed_data\n",
    "\n",
    "# Execute data processing\n",
    "processed_data = process_data(raw_data, data_processor, logger)\n",
    "\n",
    "# Display a sample of the processed data\n",
    "display(processed_data.head())\n",
    "print(f\"Processed data shape: {processed_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "777d7889",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprocessed_data\u001b[49m.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'processed_data' is not defined"
     ]
    }
   ],
   "source": [
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec556ee",
   "metadata": {},
   "source": [
    "## 3. Feature Generation\n",
    "\n",
    "Generate features for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1e20cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features\n",
    "def generate_features(processed_data, config, logger):\n",
    "    logger.info(\"Generating features...\")\n",
    "    feature_generator = FeatureGenerator(config, logger)\n",
    "    features = feature_generator.generate_features(processed_data)\n",
    "    logger.info(f\"Generated features shape: {features.shape}\")\n",
    "    return features, feature_generator\n",
    "\n",
    "# Execute feature generation\n",
    "features, feature_generator = generate_features(processed_data, config, logger)\n",
    "\n",
    "# Display a sample of the generated features\n",
    "display(features.head())\n",
    "print(f\"Features shape: {features.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320022ff",
   "metadata": {},
   "source": [
    "## 4. Feature Selection\n",
    "\n",
    "Select important features using SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c1f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "def select_features(features, config, logger):\n",
    "    logger.info(\"Selecting features...\")\n",
    "    feature_selector = FeatureSelector(config, logger)\n",
    "    \n",
    "    # Get target variables from config\n",
    "    target_vars = config['features']['targets']\n",
    "    \n",
    "    # Prepare X and y\n",
    "    X = features.drop(columns=target_vars)\n",
    "    y = features[target_vars[0]]  # Use the first target variable\n",
    "    \n",
    "    # Select features\n",
    "    selected_features = feature_selector.select_features(X, y)\n",
    "    logger.info(f\"Selected {len(selected_features)} features\")\n",
    "    \n",
    "    # Create dataset with selected features and target\n",
    "    selected_data = features[selected_features + target_vars]\n",
    "    \n",
    "    return selected_data, selected_features, feature_selector\n",
    "\n",
    "# Execute feature selection\n",
    "selected_data, selected_features, feature_selector = select_features(features, config, logger)\n",
    "\n",
    "# Display the selected features\n",
    "print(\"Selected features:\")\n",
    "print(selected_features)\n",
    "print(f\"\\nSelected data shape: {selected_data.shape}\")\n",
    "display(selected_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c660718",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation\n",
    "\n",
    "Train and evaluate regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "def train_and_evaluate_model(selected_data, config, logger):\n",
    "    logger.info(\"Training and evaluating model...\")\n",
    "    model_trainer = ModelTrainer(config, logger)\n",
    "    \n",
    "    # Get target variables from config\n",
    "    target_vars = config['features']['targets']\n",
    "    \n",
    "    # Prepare X and y\n",
    "    X = selected_data.drop(columns=target_vars)\n",
    "    y = selected_data[target_vars[0]]  # Use the first target variable\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    model, metrics = model_trainer.train_and_evaluate(X, y)\n",
    "    \n",
    "    return model, metrics, model_trainer\n",
    "\n",
    "# Execute model training and evaluation\n",
    "model, metrics, model_trainer = train_and_evaluate_model(selected_data, config, logger)\n",
    "\n",
    "# Display model metrics\n",
    "print(\"Model Metrics:\")\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    print(f\"{metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da9a97",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "Visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "def visualize_results(model, selected_data, selected_features, feature_selector, config, logger):\n",
    "    logger.info(\"Visualizing results...\")\n",
    "    visualizer = Visualizer(config, logger)\n",
    "    \n",
    "    # Get target variables from config\n",
    "    target_vars = config['features']['targets']\n",
    "    \n",
    "    # Prepare X and y\n",
    "    X = selected_data.drop(columns=target_vars)\n",
    "    y = selected_data[target_vars[0]]  # Use the first target variable\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    visualizer.plot_feature_importance(model, X.columns)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot SHAP values\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    visualizer.plot_shap_values(model, X)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    y_pred = model.predict(X)\n",
    "    visualizer.plot_actual_vs_predicted(y, y_pred)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return visualizer\n",
    "\n",
    "# Execute visualization\n",
    "visualizer = visualize_results(model, selected_data, selected_features, feature_selector, config, logger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e7d746",
   "metadata": {},
   "source": [
    "## 7. Save Results\n",
    "\n",
    "Save the processed data, features, model, and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04868aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "def save_results(processed_data, features, selected_data, model, metrics, config, logger):\n",
    "    logger.info(\"Saving results...\")\n",
    "    \n",
    "    # Get the selected mode and date\n",
    "    mode, date = get_mode_and_date()\n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = os.path.join('..', 'data', 'results', mode, date_str)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save processed data\n",
    "    processed_data_path = os.path.join(output_dir, 'processed_data.csv')\n",
    "    processed_data.to_csv(processed_data_path, index=False)\n",
    "    logger.info(f\"Saved processed data to {processed_data_path}\")\n",
    "    \n",
    "    # Save features\n",
    "    features_path = os.path.join(output_dir, 'features.csv')\n",
    "    features.to_csv(features_path, index=False)\n",
    "    logger.info(f\"Saved features to {features_path}\")\n",
    "    \n",
    "    # Save selected data\n",
    "    selected_data_path = os.path.join(output_dir, 'selected_data.csv')\n",
    "    selected_data.to_csv(selected_data_path, index=False)\n",
    "    logger.info(f\"Saved selected data to {selected_data_path}\")\n",
    "    \n",
    "    # Save model\n",
    "    import joblib\n",
    "    model_path = os.path.join(output_dir, 'model.joblib')\n",
    "    joblib.dump(model, model_path)\n",
    "    logger.info(f\"Saved model to {model_path}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    import json\n",
    "    metrics_path = os.path.join(output_dir, 'metrics.json')\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    logger.info(f\"Saved metrics to {metrics_path}\")\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "# Execute saving results\n",
    "output_dir = save_results(processed_data, features, selected_data, model, metrics, config, logger)\n",
    "print(f\"Results saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf33aee",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "Display a summary of the pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd65933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary\n",
    "def display_summary(raw_data, processed_data, features, selected_data, selected_features, metrics, config):\n",
    "    # Get the selected mode and date\n",
    "    mode, date = get_mode_and_date()\n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"DECOHERE Pipeline Summary - {mode.upper()} MODE - {date_str}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nData Processing:\")\n",
    "    print(f\"  Raw data shape: {raw_data.shape}\")\n",
    "    print(f\"  Processed data shape: {processed_data.shape}\")\n",
    "    \n",
    "    print(\"\\nFeature Engineering:\")\n",
    "    print(f\"  Total features generated: {features.shape[1] - len(config['features']['targets'])}\")\n",
    "    print(f\"  Selected features: {len(selected_features)}\")\n",
    "    \n",
    "    print(\"\\nModel Performance:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n",
    "    \n",
    "    print(\"\\nTop 10 Important Features:\")\n",
    "    for i, feature in enumerate(selected_features[:10]):\n",
    "        print(f\"  {i+1}. {feature}\")\n",
    "    \n",
    "    print(\"\\nResults saved to:\")\n",
    "    print(f\"  {os.path.abspath(output_dir)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Execute summary display\n",
    "display_summary(raw_data, processed_data, features, selected_data, selected_features, metrics, config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
