{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path.cwd().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.data.efficient_data_storage import EfficientDataStorage, DataType, DataStage\n",
    "from src.data.data_processor import DataProcessor\n",
    "from src.utils.logging_config import setup_logging\n",
    "from src.utils.config_loader import load_config\n",
    "\n",
    "from src.data.feature_generator import FeatureGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 04:21:56,217 - DECOHERE - INFO - Pipeline initialization started\n",
      "2025-04-01 04:21:56,218 - DECOHERE - INFO - Validating data structure\n",
      "2025-04-01 04:21:56,219 - DECOHERE - INFO - Data structure validation completed successfully\n",
      "2025-04-01 04:21:56,220 - DECOHERE - INFO - Validating data structure\n",
      "2025-04-01 04:21:56,220 - DECOHERE - INFO - Data structure validation completed successfully\n",
      "2025-04-01 04:21:56,221 - DECOHERE - INFO - Pipeline initialization completed\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup\n",
    "def setup_pipeline():\n",
    "    \"\"\"Initialize pipeline configuration and logging.\"\"\"\n",
    "    # Load configuration with absolute path\n",
    "    config_path = '/home/siddharth.johri/DECOHERE/config/config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    \n",
    "    # Setup logging\n",
    "    logger = setup_logging(config)\n",
    "    logger.info(\"Pipeline initialization started\")\n",
    "    \n",
    "    # Initialize storage system\n",
    "    storage = EfficientDataStorage(config, logger)\n",
    "    \n",
    "    # Initialize data processor\n",
    "    processor = DataProcessor(config, logger)\n",
    "    \n",
    "    # Initialize feature generator\n",
    "    feature_generator = FeatureGenerator(config, logger)\n",
    "    \n",
    "    logger.info(\"Pipeline initialization completed\")\n",
    "    return config, logger, storage, processor, feature_generator\n",
    "\n",
    "# Initialize pipeline\n",
    "config, logger, storage, processor, feature_generator = setup_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 04:21:56,252 - DECOHERE - INFO - Validating data structure\n",
      "2025-04-01 04:21:56,254 - DECOHERE - INFO - Data structure validation completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize data processor\n",
    "from typing import Tuple\n",
    "data_processor = DataProcessor(config, logger)\n",
    "\n",
    "def load_and_process_data(date: str, mode: str = 'day') -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and process data for a specific date and mode.\n",
    "    \n",
    "    Args:\n",
    "        date: Date to process (YYYY-MM-DD)\n",
    "        mode: Mode to use ('day', 'week', 'year')\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (processed_df, feature_ready_df)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load raw data\n",
    "        raw_df = data_processor.load_raw_data(date)\n",
    "        \n",
    "        if raw_df.empty:\n",
    "            logger.warning(f\"No data found for date: {date}\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "            \n",
    "        # Transform raw data\n",
    "        transformed_df = data_processor.transform_raw_data(raw_df)\n",
    "        \n",
    "        # Fill missing values\n",
    "        filled_df = data_processor.fill_missing_values(transformed_df)\n",
    "        \n",
    "        # Generate feature-ready DataFrame\n",
    "        feature_ready_df = data_processor.processed_data_feat_gen(filled_df)\n",
    "        \n",
    "        # Save processed data\n",
    "        processed_path = data_processor.save_processed_data(filled_df, date)\n",
    "        logger.info(f\"Saved processed data to: {processed_path}\")\n",
    "        \n",
    "        # Save feature-ready data\n",
    "        feature_path = data_processor.save_pre_feature_set(feature_ready_df)\n",
    "        logger.info(f\"Saved feature-ready data to: {feature_path}\")\n",
    "        \n",
    "        return filled_df, feature_ready_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing data for date {date}: {str(e)}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_and_process_data(date_str: str):\n",
    "#     \"\"\"Load and process data for a specific date.\"\"\"\n",
    "#     try:\n",
    "#         # Load raw data\n",
    "#         raw_data = processor.load_raw_data(date_str)\n",
    "#         if raw_data.empty:\n",
    "#             logger.warning(f\"No raw data found for date {date_str}\")\n",
    "#             return pd.DataFrame(), pd.DataFrame()\n",
    "        \n",
    "#         # Transform data\n",
    "#         transformed_data = processor.transform_data(raw_data)\n",
    "        \n",
    "#         # Fill missing values\n",
    "#         filled_data = processor.fill_missing_values(transformed_data)\n",
    "        \n",
    "#         # Generate features\n",
    "#         feature_df = feature_generator.generate_enhanced_features(\n",
    "#             filled_data,\n",
    "#             hist_window=6,\n",
    "#             fwd_window=6,\n",
    "#             target_metric='PE_RATIO_RATIO_SIGNED_LOG'\n",
    "#         )\n",
    "        \n",
    "#         if not feature_df.empty:\n",
    "#             # Save processed data\n",
    "#             storage.save_processed_data(feature_df, date_str)\n",
    "#             logger.info(f\"Saved processed data for date {date_str}\")\n",
    "            \n",
    "#             # Save feature-ready data\n",
    "#             storage.save_feature_ready_data(feature_df, date_str)\n",
    "#             logger.info(f\"Saved feature-ready data for date {date_str}\")\n",
    "            \n",
    "#             return feature_df, filled_data\n",
    "#         else:\n",
    "#             logger.warning(f\"No features generated for date {date_str}\")\n",
    "#             return pd.DataFrame(), pd.DataFrame()\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error processing data for date {date_str}: {str(e)}\")\n",
    "#         return pd.DataFrame(), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(processed_df: pd.DataFrame, mode: str = 'day') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate features from processed data.\n",
    "    \n",
    "    Args:\n",
    "        processed_df: DataFrame containing processed data\n",
    "        mode: Mode to use ('day', 'week', 'year')\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing generated features\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate feature set from processed data\n",
    "        feature_df = data_processor.processed_data_feat_gen(processed_df)\n",
    "        \n",
    "        if feature_df.empty:\n",
    "            logger.warning(\"No features generated\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        # Save pre-feature set\n",
    "        pre_feature_path = data_processor.save_pre_feature_set(feature_df)\n",
    "        logger.info(f\"Saved pre-feature set to: {pre_feature_path}\")\n",
    "        \n",
    "        return feature_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating features: {str(e)}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(date_str: str):\n",
    "    \"\"\"Load and process data for a specific date.\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nStarting data processing for {date_str}\")\n",
    "        \n",
    "        # Load raw data\n",
    "        print(\"Loading raw data...\")\n",
    "        raw_data = processor.load_raw_data(date_str)\n",
    "        if raw_data.empty:\n",
    "            logger.warning(f\"No raw data found for date {date_str}\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "        print(f\"Raw data shape: {raw_data.shape}\")\n",
    "        \n",
    "        # Transform data\n",
    "        print(\"Transforming data...\")\n",
    "        transformed_data = processor.transform_raw_data(raw_data)  # Changed from transform_data to transform_raw_data\n",
    "        print(f\"Transformed data shape: {transformed_data.shape}\")\n",
    "        \n",
    "        # Fill missing values\n",
    "        print(\"Filling missing values...\")\n",
    "        filled_data = processor.fill_missing_values(transformed_data)\n",
    "        print(f\"Filled data shape: {filled_data.shape}\")\n",
    "        \n",
    "        # Generate features\n",
    "        print(\"Generating features...\")\n",
    "        feature_df = feature_generator.generate_enhanced_features(\n",
    "            filled_data,\n",
    "            hist_window=6,\n",
    "            fwd_window=6,\n",
    "            target_metric='PE_RATIO_RATIO_SIGNED_LOG'\n",
    "        )\n",
    "        print(f\"Feature data shape: {feature_df.shape}\")\n",
    "        \n",
    "        if not feature_df.empty:\n",
    "            # Save processed data\n",
    "            print(f\"Saving processed data for {date_str}...\")\n",
    "            storage.store_data(\n",
    "                df=feature_df,\n",
    "                data_type=DataType.FUNDAMENTALS,\n",
    "                stage=DataStage.PROCESSED,\n",
    "                date=date_str\n",
    "            )\n",
    "            \n",
    "            # Save feature-ready data\n",
    "            print(f\"Saving feature-ready data for {date_str}...\")\n",
    "            storage.store_data(\n",
    "                df=feature_df,\n",
    "                data_type=DataType.FUNDAMENTALS,\n",
    "                stage=DataStage.FEATURES,\n",
    "                date=date_str,\n",
    "                sub_type='pre_feature_set'\n",
    "            )\n",
    "            \n",
    "            return feature_df, filled_data\n",
    "        else:\n",
    "            logger.warning(f\"No features generated for date {date_str}\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing data for date {date_str}: {str(e)}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(date_str: str):\n",
    "    \"\"\"Load and process data for a specific date.\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nStarting data processing for {date_str}\")\n",
    "        \n",
    "        # Load raw data\n",
    "        print(\"Loading raw data...\")\n",
    "        raw_data = processor.load_raw_data(date_str)\n",
    "        if raw_data.empty:\n",
    "            logger.warning(f\"No raw data found for date {date_str}\")\n",
    "            return pd.DataFrame()\n",
    "        print(f\"Raw data shape: {raw_data.shape}\")\n",
    "        \n",
    "        # Transform data\n",
    "        print(\"Transforming data...\")\n",
    "        transformed_data = processor.transform_raw_data(raw_data)\n",
    "        print(f\"Transformed data shape: {transformed_data.shape}\")\n",
    "        \n",
    "        # Fill missing values\n",
    "        print(\"Filling missing values...\")\n",
    "        filled_data = processor.fill_missing_values(transformed_data)\n",
    "        print(f\"Filled data shape: {filled_data.shape}\")\n",
    "        \n",
    "        # Print available columns for debugging\n",
    "        print(\"\\nAvailable columns in processed data:\")\n",
    "        for col in filled_data.columns:\n",
    "            if any(pattern in col.lower() for pattern in ['signed_log', 'ratio', 'coeff_of_var']):\n",
    "                print(f\"- {col}\")\n",
    "        \n",
    "        # Save processed data\n",
    "        print(f\"\\nSaving processed data for {date_str}...\")\n",
    "        storage.store_data(\n",
    "            df=filled_data,\n",
    "            data_type=DataType.FUNDAMENTALS,\n",
    "            stage=DataStage.PROCESSED,\n",
    "            date=date_str\n",
    "        )\n",
    "        \n",
    "        # Generate and save pre-feature set data\n",
    "        print(f\"Generating and saving pre-feature set data for {date_str}...\")\n",
    "        pre_feature_data = storage.processed_data_feat_gen(filled_data)\n",
    "        if not pre_feature_data.empty:\n",
    "            storage.store_data(\n",
    "                df=pre_feature_data,\n",
    "                data_type=DataType.FUNDAMENTALS,\n",
    "                stage=DataStage.FEATURES,\n",
    "                date=date_str,\n",
    "                sub_type='pre_feature_set'\n",
    "            )\n",
    "            print(f\"Pre-feature set data shape: {pre_feature_data.shape}\")\n",
    "            print(\"Pre-feature set columns:\")\n",
    "            for col in pre_feature_data.columns:\n",
    "                print(f\"- {col}\")\n",
    "        else:\n",
    "            print(\"Warning: No pre-feature set data generated\")\n",
    "        \n",
    "        return filled_data\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing data for date {date_str}: {str(e)}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_enhanced_features(date_str: str, processed_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate enhanced features for a specific date using the FeatureGenerator.\n",
    "    \n",
    "    Args:\n",
    "        date_str: Date string in YYYY-MM-DD format\n",
    "        processed_df: DataFrame containing processed data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing generated features\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate features\n",
    "        feature_df = feature_generator.generate_enhanced_features(\n",
    "            processed_df,\n",
    "            hist_window=6,\n",
    "            fwd_window=6,\n",
    "            target_metric='PE_RATIO_RATIO_SIGNED_LOG'\n",
    "        )\n",
    "        \n",
    "        if not feature_df.empty:\n",
    "            print(f\"\\nGenerated features for {date_str}:\")\n",
    "            print(f\"Feature DataFrame shape: {feature_df.shape}\")\n",
    "            print(\"\\nSample of features:\")\n",
    "            print(feature_df.head())\n",
    "            return feature_df\n",
    "        else:\n",
    "            print(f\"\\nNo features generated for {date_str}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating features for date {date_str}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example usage:\n",
    "# processed_df, _ = load_and_process_data(\"2024-09-02\")\n",
    "# feature_df = generate_enhanced_features(\"2024-09-02\", processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process multiple dates\n",
    "# dates = [\"2024-09-02\", \"2024-09-03\", \"2024-09-04\"]\n",
    "\n",
    "# for date in dates:\n",
    "#     # Load and process data\n",
    "#     processed_df, _ = load_and_process_data(date)\n",
    "    \n",
    "#     if not processed_df.empty:\n",
    "#         # Generate features\n",
    "#         feature_df = generate_enhanced_features(date, processed_df)\n",
    "        \n",
    "#         if not feature_df.empty:\n",
    "#             print(f\"\\nSuccessfully generated features for {date}\")\n",
    "#         else:\n",
    "#             print(f\"\\nFailed to generate features for {date}\")\n",
    "#     else:\n",
    "#         print(f\"\\nNo data processed for {date}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
